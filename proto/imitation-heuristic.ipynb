{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4a90ea",
   "metadata": {},
   "source": [
    "# A dqn agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7505b82",
   "metadata": {},
   "source": [
    "The pp-curve drawing procedure. We compare distibutions using the pp-curve, which is analogous to the ROC curve: pp compares two independent distributions, while ROC compares the true-, false- positive distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "\n",
    "from toybnb.scip.ecole.il.plotting import pp_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec7c79",
   "metadata": {},
   "source": [
    "A simple viz for tracking loss and other runtime series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.il.plotting import plot_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fbc898",
   "metadata": {},
   "source": [
    "SeedSequence needs a `.spawn-one` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng, SeedSequence\n",
    "\n",
    "\n",
    "def spawn_one(ss: SeedSequence) -> SeedSequence:\n",
    "    return ss.spawn(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b39ce",
   "metadata": {},
   "source": [
    "A function to add a dict record to a table (dict-of-lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def do_add(\n",
    "    record: dict, to: dict[..., list], transform: Callable[..., dict] = None\n",
    ") -> dict:\n",
    "    \"\"\"Add the record to a transposed dict of lists.\"\"\"\n",
    "    original = record\n",
    "    if callable(transform):\n",
    "        record = transform(**record)\n",
    "\n",
    "    # assume no fields are missing\n",
    "    for field, value in record.items():\n",
    "        to.setdefault(field, []).append(value)\n",
    "\n",
    "    return original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a24199",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17756684",
   "metadata": {},
   "source": [
    "## Composing the datastreams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7435de2",
   "metadata": {},
   "source": [
    "Reservoir sampling for data from infinite data streams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c28ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def shuffle(\n",
    "    it: Iterable,\n",
    "    n_size: int = 1024,\n",
    "    seed: int = None,\n",
    ") -> Iterable:\n",
    "    \"\"\"Shuffle the values from the iterable\"\"\"\n",
    "\n",
    "    rng, reservoir = default_rng(seed), []\n",
    "    for sample in it:\n",
    "        # stack elements until the reservior is full\n",
    "        if len(reservoir) < n_size:\n",
    "            reservoir.append(sample)\n",
    "            continue\n",
    "\n",
    "        # replace a random element with the new sample\n",
    "        ix = rng.choice(n_size)\n",
    "        yield reservoir[ix]\n",
    "        reservoir[ix] = sample\n",
    "\n",
    "    # re-shuffle the remaining samples (in the case\n",
    "    #  the sequence was too short for proper mixing)\n",
    "    rng.shuffle(reservoir)\n",
    "    yield from reservoir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3907b9",
   "metadata": {},
   "source": [
    "Batch the data in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(it: Iterable, n_size: int = 16) -> Iterable[list]:\n",
    "    \"\"\"Batch the values from the iterable\"\"\"\n",
    "    batch = []\n",
    "    for sample in it:\n",
    "        batch.append(sample)\n",
    "\n",
    "        # produce the batch when it's full and then clear\n",
    "        if len(batch) >= n_size:\n",
    "            yield batch\n",
    "            batch.clear()\n",
    "\n",
    "    # don't forget the residual batch\n",
    "    if batch:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a1863",
   "metadata": {},
   "source": [
    "a handy sequence limiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "def limit(it: Iterable, limiter: Union[int, Iterable]) -> Iterable:\n",
    "    \"\"\"Limit the length of the sequence to at most `n_total` values\"\"\"\n",
    "    limiter = range(limiter) if isinstance(limiter, int) else limiter\n",
    "    for sample, _ in zip(it, limiter):\n",
    "        yield sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d757b10",
   "metadata": {},
   "source": [
    "finally, a generator that mixes data from multiple iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixer(*its: Iterable, seed: int = None) -> Iterable:\n",
    "    \"\"\"Yield a value from an iterable picked at random each time\"\"\"\n",
    "    iters = list(map(iter, its))\n",
    "\n",
    "    rng = default_rng(seed)\n",
    "    while iters:\n",
    "        it = rng.choice(iters, shuffle=False)\n",
    "        try:\n",
    "            yield next(it)\n",
    "\n",
    "        except StopIteration:\n",
    "            iters.remove(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13fc53",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddeff38",
   "metadata": {},
   "source": [
    "## CO and Branching data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ffac7a",
   "metadata": {},
   "source": [
    "The following SCIP settings were inherited from [Gasse et al. 2019](), [Parsonson et al. 2022](), and\n",
    "[Scavuzzo et al. 2022]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyscipopt\n",
    "import ecole as ec\n",
    "\n",
    "# from toybnb.scip.ecole.il.env import default_scip_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446ff20",
   "metadata": {},
   "source": [
    "A derived branching env that disables SCIP's presolver\n",
    "- without presolve training becomes much slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491390b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecole.environment import Branching\n",
    "\n",
    "# from toybnb.scip.ecole.il.env import BranchingWithoutPresolve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ace826",
   "metadata": {},
   "source": [
    "We want to train a problem-dependent branching heuristic. Strong branching and pseudocost have the best problem dependency -- they have access to its geometry, but are slow. Our goal is to train a neural net that extract geometric info from the parametric representation of a problem at a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5222c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecole.observation import NodeBipartite\n",
    "from ecole.reward import Constant\n",
    "\n",
    "\n",
    "from toybnb.scip.ecole.il.env import make_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dce7df9",
   "metadata": {},
   "source": [
    "We use a special representation of a batch of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from ecole.observation import NodeBipartiteObs\n",
    "from torch import Tensor\n",
    "from numpy import ndarray\n",
    "\n",
    "\n",
    "from toybnb.scip.ecole.il.data import Observation, BatchObservation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6b09c",
   "metadata": {},
   "source": [
    "A special collation logic for `Observation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import as_tensor\n",
    "from toybnb.scip.ecole.il.data import collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613387e",
   "metadata": {},
   "source": [
    "We source supervised data from the strong branching heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4accacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecole.observation import StrongBranchingScores, Pseudocosts\n",
    "from ecole.core.scip import Model, Stage\n",
    "\n",
    "from toybnb.scip.ecole.il.brancher import BranchRule, BranchRuleCallable\n",
    "from toybnb.scip.ecole.il.brancher import strongbranch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af75118",
   "metadata": {},
   "source": [
    "We also compare to a randombranching expert, although its utility is vague."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5514bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.il.brancher import randombranch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb081e3",
   "metadata": {},
   "source": [
    "And, finally, a function to use the trained machine learning model to pick branching vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "\n",
    "from toybnb.scip.ecole.il.brancher import batched_ml_branchrule, ml_branchrule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b700c7c",
   "metadata": {},
   "source": [
    "A branchrule that communicates with a central action server, that attempts to process the requests in batches for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread, Event\n",
    "from queue import Queue, Empty as QueueEmpty\n",
    "\n",
    "from toybnb.scip.ecole.il.threads import BatchProcessor\n",
    "\n",
    "\n",
    "class BranchingServer(BatchProcessor):\n",
    "    \"\"\"Branching variable Server\"\"\"\n",
    "\n",
    "    def connect(self, env: Branching) -> BranchRuleCallable:\n",
    "        \"\"\"Spawn a new branchrule\"\"\"\n",
    "        co_yield = super().connect()\n",
    "\n",
    "        def _branchrule(obs: Observation, **ignored: dict) -> int:\n",
    "            if env.model.stage != Stage.Solving:\n",
    "                return None\n",
    "\n",
    "            return int(co_yield(obs))\n",
    "\n",
    "        return _branchrule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2d86e",
   "metadata": {},
   "source": [
    "A procedure to seed Ecole's PRNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d48a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecole import RandomGenerator\n",
    "\n",
    "from toybnb.scip.ecole.il.env import ecole_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e79622",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc5f0f",
   "metadata": {},
   "source": [
    "## The data source proper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c52e76",
   "metadata": {},
   "source": [
    "During training we need a generator of observation, action, and reward data from the decision nodes of the BnB tree. During evaluation we only care about the final `nfo` data from the branching environment, as it contains the post-search tree stats. `rollout` returns both kinds of data, `evaluate` is a handy wrapper around `rollout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.il.rollout import rollout, evaluate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9aab5e9c",
   "metadata": {},
   "source": [
    "co_it = ec.instance.CombinatorialAuctionGenerator()\n",
    "\n",
    "data = []\n",
    "for obs, act, rew in rollout(next(co_it), make_env(), strongbranch(), {}, None):\n",
    "    data.append((obs, act, rew))\n",
    "\n",
    "evaluate(next(co_it), make_env(), strongbranch())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e636de",
   "metadata": {},
   "source": [
    "We use infinite problem generators, for which we implement a continuous rollout wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2af336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_rollout(\n",
    "    it: Iterable[Model], env: Branching, branchrule: BranchRule, kwargs: dict = None\n",
    ") -> Iterable:\n",
    "    # we use for-loop in case an infinite generator is actually finite\n",
    "    for p in it:\n",
    "        yield from rollout(p, env, branchrule, kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d1985",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d1c03",
   "metadata": {},
   "source": [
    "### A multithreaded version for especially slow problem instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45aacfa",
   "metadata": {},
   "source": [
    "The inner logic of the parallel job feeder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54797c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Empty, Queue, Full\n",
    "from threading import Event\n",
    "\n",
    "\n",
    "def t_feed(\n",
    "    it: Iterable,\n",
    "    to: Queue,\n",
    "    *,\n",
    "    signal: Event,\n",
    "    err: Queue,\n",
    "    timeout: float = 1.0,\n",
    ") -> None:\n",
    "    \"\"\"Keep putting items from the iterable into the queue, until stopped\"\"\"\n",
    "    try:\n",
    "        item = next(it)\n",
    "        while not signal.is_set():\n",
    "            try:\n",
    "                to.put(item, True, timeout)\n",
    "            except Full:\n",
    "                continue\n",
    "\n",
    "            item = next(it)\n",
    "\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "    except BaseException as e:\n",
    "        err.put_nowait(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad3794",
   "metadata": {},
   "source": [
    "The body of a parallel rollout worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8769907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def t_rollout(\n",
    "    feed: Queue,\n",
    "    factory: Callable,\n",
    "    branchrule: Callable,\n",
    "    into: Queue,\n",
    "    *,\n",
    "    signal: Event,\n",
    "    err: Queue,\n",
    "    timeout: float = 1.0,\n",
    ") -> None:\n",
    "    \"\"\"Rollout the `branchrule` on instances from `feed` solved in `factory`,\n",
    "    saving the observations in `into`\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # get the env\n",
    "        env = factory()\n",
    "        while not signal.is_set():\n",
    "            # poll the feed queue for a new job\n",
    "            try:\n",
    "                p = feed.get(True, timeout)\n",
    "            except Empty:\n",
    "                continue\n",
    "\n",
    "            # do a rollout, sending the results into the buffer\n",
    "            for item in rollout(p, env, branchrule, {}, stop=signal.is_set):\n",
    "                while not signal.is_set():\n",
    "                    try:\n",
    "                        into.put(item, True, timeout)\n",
    "                    except Full:\n",
    "                        continue\n",
    "\n",
    "                    break\n",
    "\n",
    "    except BaseException as e:\n",
    "        err.put_nowait(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4dd38",
   "metadata": {},
   "source": [
    "The procedure itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef705818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def maybe_raise(err: Queue) -> None:\n",
    "    \"\"\"Raise if the error queue has an exception\"\"\"\n",
    "    with err.mutex:\n",
    "        if err.queue:\n",
    "            raise err.queue.popleft()\n",
    "\n",
    "\n",
    "def multirollout(\n",
    "    feed: Iterable,\n",
    "    ss: SeedSequence,\n",
    "    branchrule: BranchRule,\n",
    "    n_jobs: int = 1,\n",
    ") -> Iterable:\n",
    "    if n_jobs < 2:\n",
    "        (fork,) = ss.spawn(1)\n",
    "        yield from continuous_rollout(feed, make_env(fork), branchrule, {})\n",
    "        return\n",
    "\n",
    "    ctx = dict(signal=Event(), err=Queue(), timeout=1.0)\n",
    "\n",
    "    # spawn feed thread and the workers\n",
    "    feed_q, rollout_q = Queue(32), Queue()\n",
    "    threads = [Thread(target=t_feed, args=(feed, feed_q), kwargs=ctx, daemon=True)]\n",
    "    for fork in ss.spawn(n_jobs):\n",
    "        args = feed_q, partial(make_env, fork), branchrule, rollout_q\n",
    "        threads.append(Thread(target=t_rollout, args=args, kwargs=ctx, daemon=True))\n",
    "\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "\n",
    "    try:\n",
    "        # the main thread yields results from the rollout output queue\n",
    "        while True:\n",
    "            maybe_raise(ctx[\"err\"])\n",
    "            try:\n",
    "                yield rollout_q.get(True, timeout=5.0)\n",
    "\n",
    "            except Empty:\n",
    "                continue\n",
    "\n",
    "    finally:\n",
    "        ctx[\"signal\"].set()\n",
    "        for t in threads:\n",
    "            t.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70534898",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141b17b",
   "metadata": {},
   "source": [
    "## the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac1a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782cb876",
   "metadata": {},
   "source": [
    "A tracer hook to help with debugging live models\n",
    "\n",
    "```python\n",
    "hook = model.register_forward_pre_hook(tracer)\n",
    "\n",
    "bt, by = next(feed)\n",
    "model(bt, by)\n",
    "\n",
    "...\n",
    "\n",
    "hook.remove()\n",
    "```\n",
    "- _judicious placement_ of breakpoints `b` is advised so that continuation `c` would work\n",
    "    - try `n`, `b 1208`\n",
    "- otherwise use `n` for next, `s` for step inside, `u/d` to move between stack frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracer(module, input) -> None:\n",
    "    import pdb\n",
    "\n",
    "    pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def71b8",
   "metadata": {},
   "source": [
    "A good old trusted MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.il.nn import mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1663a59",
   "metadata": {},
   "source": [
    "The original message passing architecture is\n",
    "$$\n",
    "x^k_i\n",
    "    = \\gamma\\bigl(\n",
    "        x^{k-1}_i,\n",
    "        \\operatorname{\\diamond} \\bigl(\n",
    "            \\bigl\\{\n",
    "                \\phi(x^{k-1}_i, x^{k-1}_j, e_{ij})\n",
    "                \\colon j \\in G_i\n",
    "            \\bigr\\}\n",
    "        \\bigr)\n",
    "    \\bigr)\n",
    "    \\,, $$\n",
    "\n",
    "where $\\diamond$ is a permutation-invariant set-to-real _aggregation_ operator,\n",
    "$\\phi$ is the _message_ function, and $\\gamma$ is the _output_ function, and $G_u$\n",
    "is the graph neighborhood of the vertex $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97994190",
   "metadata": {},
   "source": [
    "Below we implement a variant of the massage passing where the _aggregation_ and _message_ operations are based on multi-headed cross attention. This means, that $\\diamond$ and $\\phi$ are _fused_:\n",
    "$$\n",
    "    (\\diamond \\circ \\phi)\n",
    "    \\colon \\mathbb{R}^d \\times \\bigl(\\mathbb{R}^d\\bigr)^* \\to \\mathbb{R}^d\n",
    "    \\colon (x_i, \\{x_j\\colon j \\in G_i\\}) \\mapsto\n",
    "        \\sum_{j \\in G_i} p_j \\phi^v(x_j)\n",
    "    \\,. $$\n",
    "with $\\log p_j \\propto \\phi^q(x_i)^\\top \\phi^k(x_j)$ for $j \\in G_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f31923",
   "metadata": {},
   "source": [
    "Implement a special layer that computes message passing with cross-attention between the parts of a bipartite graph. We make heavy use of `torch-scatter` operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1347483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "from einops import rearrange\n",
    "from torch_scatter import scatter_softmax, scatter_sum\n",
    "\n",
    "# from toybnb.scip.ecole.il.nn import BipartiteMHXA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70273f92",
   "metadata": {},
   "source": [
    "A bipartite transformer block\n",
    "* we opt for post-norm architecture\n",
    "* [Zhang et al. (2022)](https://arxiv.org/abs/2206.11925.pdf) metnion that norm-first layer norm may impact expressivty, but do not analyze when the norm is at the other end\n",
    "* while [](https://arxiv.org/abs/2002.04745.pdf) claim that pre-LN stabilizes the grad, whereas post-LN expected grads' magnitude grows with the depth\n",
    "  - strange, but this appears to contradict their own theoreical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe48bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from toybnb.scip.ecole.il.nn import BipartiteBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce1711",
   "metadata": {},
   "source": [
    "The full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_log_softmax, scatter_logsumexp\n",
    "from torch_scatter import scatter_max, scatter_mean\n",
    "\n",
    "from toybnb.scip.ecole.il.nn import NeuralVariableSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5962ccd4",
   "metadata": {},
   "source": [
    "The network from Gasse et al. 2019 with PreNorm layers replaced by batchnorms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9965e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.il.nn import BipartiteGConv, NeuralClassifierBranchruleMixin\n",
    "\n",
    "\n",
    "class Gasse2019(Module, NeuralClassifierBranchruleMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dim_vars: int = 19,\n",
    "        n_dim_cons: int = 5,\n",
    "        n_embed: int = 64,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleDict(\n",
    "            dict(\n",
    "                vars=nn.Sequential(\n",
    "                    # PreNormLayer,\n",
    "                    # nn.BatchNorm1d(n_dim_vars, affine=False),\n",
    "                    nn.Linear(n_dim_vars, n_embed),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_embed, n_embed),\n",
    "                    nn.ReLU(),\n",
    "                ),\n",
    "                cons=nn.Sequential(\n",
    "                    # PreNormLayer,\n",
    "                    # nn.BatchNorm1d(n_dim_cons, affine=False),\n",
    "                    nn.Linear(n_dim_cons, n_embed),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_embed, n_embed),\n",
    "                    nn.ReLU(),\n",
    "                ),\n",
    "                edge=nn.Sequential(\n",
    "                    # PreNormLayer,\n",
    "                    nn.Unflatten(-1, (-1, 1)),\n",
    "                    # nn.BatchNorm1d(1, affine=False),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.v2c = BipartiteGConv(n_embed, True)\n",
    "        self.c2v = BipartiteGConv(n_embed, True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input: BatchObservation) -> Tensor:\n",
    "        jc, jv = input.ctov_ij\n",
    "\n",
    "        # encode the vars and cons features\n",
    "        cons = self.encoder.cons(input.cons)\n",
    "        edge = self.encoder.edge(input.ctov_v)\n",
    "        vars = self.encoder.vars(input.vars)\n",
    "\n",
    "        # bipartite vcv ladder\n",
    "        # (v_{t-1}, c_{t-1}) -->> (v_{t-1}, c_t) -->> (v_t, c_t)\n",
    "        cons = self.c2v(cons, vars, (jc, jv), edge)\n",
    "        vars = self.v2c(vars, cons, (jv, jc), edge)\n",
    "\n",
    "        # get the raw-logit scores of each variable\n",
    "        return self.head(vars).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce3f4da",
   "metadata": {},
   "source": [
    "Some procs for handling dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99866c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_dict(dict: dict[..., dict]) -> dict[..., dict]:\n",
    "    outer = {}\n",
    "    for k_out, inner in dict.items():\n",
    "        for k_in, value in inner.items():\n",
    "            new = outer.setdefault(k_in, {})\n",
    "            assert k_out not in new\n",
    "            new[k_out] = value\n",
    "\n",
    "    return outer\n",
    "\n",
    "\n",
    "def collate_dict(records: list[dict]) -> dict[..., list]:\n",
    "    \"\"\"Collate records assuming no fields are missing\"\"\"\n",
    "    out = {}\n",
    "    for record in records:\n",
    "        for field, value in record.items():\n",
    "            out.setdefault(field, []).append(value)\n",
    "\n",
    "    return {k: np.array(v) for k, v in out.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee12112",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd57a29",
   "metadata": {},
   "source": [
    "## Trainnig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c5c51",
   "metadata": {},
   "source": [
    "Allow for 100k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d323ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total, n_batch_size = 100_000, 16  # XXX 100k is too long\n",
    "n_reservoir = 512  # XXX 128 reservoir was ok for CAuc\n",
    "\n",
    "C_neg_actset = 0.0  # 1e-3  # XXX the orginal CAuc used to have 0.0\n",
    "C_entropy = 0.0  # 1e-2  # XXX was set to zero in the first CAuc\n",
    "\n",
    "# use a seed sequence with a fixed entropy pool\n",
    "ss = SeedSequence(83278314352113072500167414370310027453)\n",
    "# ss = SeedSequence(None)  # use `ss.entropy` for future reproducibility\n",
    "\n",
    "s_project_graph = \"pre\"  # \"post\"\n",
    "\n",
    "p_drop = 0.2\n",
    "n_embed, n_heads, n_blocks = 32, 1, 1\n",
    "b_edges = True\n",
    "b_norm_first = True  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe36b8f",
   "metadata": {},
   "source": [
    "Pipe the generator that mixes several the CO problems into the continupus rollout iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.benchmarks import gasse2019\n",
    "from ecole.instance import CombinatorialAuctionGenerator\n",
    "\n",
    "\n",
    "# init the branching env\n",
    "env = make_env(spawn_one(ss))\n",
    "\n",
    "# CAuc(100, 500), CAuc(50, 250)\n",
    "gens = [\n",
    "    CombinatorialAuctionGenerator(100, 500, rng=ecole_seed(spawn_one(ss))),\n",
    "    CombinatorialAuctionGenerator(50, 250, rng=ecole_seed(spawn_one(ss))),\n",
    "]\n",
    "\n",
    "# Use co problems from Gasse 2019\n",
    "# gens = transpose_dict(gasse2019(spawn_one(ss)))\n",
    "# gens = gens[\"train\"].values()\n",
    "itco = mixer(*gens, seed=spawn_one(ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c4d22",
   "metadata": {},
   "source": [
    "set up the rollout observation feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = multirollout(\n",
    "    itco,\n",
    "    ss,\n",
    "    strongbranch(False),\n",
    "    n_jobs=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0734367",
   "metadata": {},
   "source": [
    "Then feed the branching observation data into a shuffler, limiter and then batcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f6118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "# set up the data stream\n",
    "# XXX if we put a limiter on the source iter, then shuffle's reservoir\n",
    "#  would consume `n_reservoir`, which are never going to be yield,\n",
    "#  if the source is infinite. In this case, instead, we should put\n",
    "#  a limiter on the `shuffle` itself.\n",
    "it = feed\n",
    "it = limit(it, trange(n_total, ncols=70))\n",
    "it = shuffle(it, n_reservoir, spawn_one(ss))\n",
    "it = batch(it, n_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858e728",
   "metadata": {},
   "source": [
    "Set up the model and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import SequentialLR\n",
    "from torch.optim.lr_scheduler import LinearLR, ConstantLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "mod = NeuralVariableSelector(\n",
    "    19,\n",
    "    5,\n",
    "    n_embed,\n",
    "    n_heads,\n",
    "    n_blocks,\n",
    "    p_drop,\n",
    "    b_norm_first=b_norm_first,\n",
    "    s_project_graph=s_project_graph,\n",
    "    b_edges=b_edges,\n",
    ")\n",
    "\n",
    "# mod = Gasse2019()\n",
    "\n",
    "# optim = torch.optim.AdamW(mod.parameters(), lr=3e-4)\n",
    "optim = torch.optim.AdamW(mod.parameters(), lr=1e-3)\n",
    "\n",
    "sched = None\n",
    "sched = SequentialLR(\n",
    "    optim,\n",
    "    [\n",
    "        ConstantLR(optim, factor=0.5, total_iters=50),\n",
    "        LinearLR(optim, start_factor=0.5, total_iters=250),\n",
    "        CosineAnnealingLR(optim, T_max=50, eta_min=1e-6),\n",
    "    ],\n",
    "    [50, 300],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c425f",
   "metadata": {},
   "source": [
    "train an IL model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24f0c1b4",
   "metadata": {},
   "source": [
    "hk = mod.register_forward_pre_hook(tracer)\n",
    "hk.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f1171",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "log = dict()\n",
    "for bt in it:\n",
    "    # collate the batch\n",
    "    obs, act, rew = zip(*bt)\n",
    "    bx = collate(obs, device=device)\n",
    "    act = as_tensor(np.array(act, dtype=int), device=device)\n",
    "    rew = np.array(rew, dtype=np.float32)\n",
    "\n",
    "    # forward pass\n",
    "    mod.train()\n",
    "    _, terms = mod.compute(bx, target=act)\n",
    "    terms = {k: v.mean() for k, v in terms.items()}\n",
    "\n",
    "    # log likelihoods and entropy have the same uints (and thus scale)\n",
    "    loss = (\n",
    "        # (min) -ve log-likelihood\n",
    "        terms[\"neg_target\"]\n",
    "        # (max) entropy\n",
    "        - C_entropy * terms[\"entropy\"]\n",
    "        # (max) -ve log-likelihood of forbidden actions\n",
    "        - C_neg_actset * terms[\"neg_actset\"]\n",
    "    )\n",
    "\n",
    "    # backprop\n",
    "    mod.zero_grad(True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if sched is not None:\n",
    "        sched.step()\n",
    "\n",
    "    mod.eval()\n",
    "    do_add({k: float(v) for k, v in terms.items()}, log)\n",
    "\n",
    "    clear_output(True)\n",
    "    fig, ax0 = plt.subplots(1, 1, figsize=(5, 2), dpi=200, sharex=True)\n",
    "    plot_series(ax0, **log)\n",
    "    ax0.set_title(\"terms\")\n",
    "    ax0.legend(loc=\"lower left\", fontsize=\"xx-small\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41875b1c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime\n",
    "\n",
    "torch.save(\n",
    "    dict(\n",
    "        __dttm__=strftime(\"%Y%m%d-%H%M%S\"),\n",
    "        state_dict=mod.state_dict(),\n",
    "    ),\n",
    "    \"ninth-cauc-norm-first.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08278268",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7d31f",
   "metadata": {},
   "source": [
    "$$\n",
    "(n+m) \\mu_{n+m} = m \\nu_m + n \\mu_n\n",
    "    \\,. $$\n",
    "$$\n",
    "\\begin{align}\n",
    "    (n+m) S^2_{n+m}\n",
    "        &= \\sum_{j < m} (x_{n+j} - \\mu_{n+m})^2 + \\sum_{j < n} (x_j - \\mu_{n+m})^2\n",
    "        \\\\\n",
    "        &= (n+m) (\\mu_n - \\mu_{n+m})^2\n",
    "        + \\sum_{j < m} (x_{n+j} - \\mu_n)^2 + \\sum_{j < n} (x_j - \\mu_n)^2\n",
    "        + 2 (\\mu_n - \\mu_{n+m}) \\sum_{j < m} (x_{n+j} - \\mu_n)\n",
    "        \\\\\n",
    "        &= (n+m) S^2_n\n",
    "        + (n+m) (\\mu_n - \\mu_{n+m})^2\n",
    "        - m S^2_n\n",
    "        + 2 m (\\mu_n - \\mu_{n+m}) (\\nu_m - \\mu_n)\n",
    "        + \\sum_{j < m} (x_{n+j} - \\mu_n)^2\n",
    "        \\,. \\\\\n",
    "    (n+m) S^2_{n+m} - n S^2_n\n",
    "        &= n (\\mu_n - \\mu_{n+m})^2\n",
    "        + m (\\nu_m - \\mu_{n+m})^2\n",
    "        + m T^2_m\n",
    "        \\\\\n",
    "        &= n (\\frac{m}{n+m} \\nu_m - \\frac{m}{n+m} \\mu_n)^2\n",
    "        + m (\\frac{-n}{n+m} \\nu_m + \\frac{n}{n+m} \\mu_n)^2\n",
    "        + m T^2_m\n",
    "        \\\\\n",
    "        &= \\frac{m n}{n+m} (\\nu_m - \\mu_n)^2 + m T^2_m\n",
    "        \\,. \\\\\n",
    "    \\mu_{n+m} - \\mu_n\n",
    "        &= \\frac{m}{n+m} (\\nu_m - \\mu_n)\n",
    "        = \\frac{m}{n+m} \\delta\n",
    "        \\\\\n",
    "    S^2_{n+m} - S^2_n\n",
    "        &= \\frac{m n}{(n+m)^2} (\\nu_m - \\mu_n)^2 + \\frac{m}{n+m} (T^2_m - S^2_n)\n",
    "        \\\\\n",
    "        &= \\frac{m}{n+m} \\Bigl(\n",
    "            \\Bigl(1 - \\frac{m}{n+m}\\Bigr) \\delta^2 + (T^2_m - S^2_n)\n",
    "        \\Bigr)\n",
    "        \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00076e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_log_softmax, scatter_logsumexp\n",
    "from torch_scatter import scatter_max, scatter_mean\n",
    "\n",
    "\n",
    "class PreNormLayer(Module):\n",
    "    \"\"\"Not quite batch norm\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int = 0,\n",
    "        location: bool = False,\n",
    "        scale: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.dim, self.location, self.scale = dim, location, scale\n",
    "        self.register_buffer(\"n_samples\", torch.tensor(0, dtype=int))\n",
    "\n",
    "        self.register_buffer(\"weight\", torch.ones(0))\n",
    "        self.register_buffer(\"bias\", torch.zeros(0))\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def _update(self, input: Tensor) -> Tensor:\n",
    "        self.n_samples += len(input)\n",
    "\n",
    "        # online average\n",
    "        nu = input.mean(self.dim)\n",
    "        if self.bias.numel() < 1:\n",
    "            self.bias.resize_as_(nu).zero_()\n",
    "\n",
    "        ratio = len(input) / int(self.n_samples)\n",
    "        delta = nu - self.bias\n",
    "        self.bias.add_(delta, alpha=ratio)\n",
    "\n",
    "        # biased online variance\n",
    "        T2 = input.var(self.dim, unbiased=False)\n",
    "        if self.weight.numel() < 1:\n",
    "            self.weight.resize_as_(T2).zero_()\n",
    "\n",
    "        sigma = (1 - ratio) * delta * delta + (T2 - self.weight)\n",
    "        self.weight.add_(sigma, alpha=ratio)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        if self.location:\n",
    "            input = input.sub(self.bias)\n",
    "\n",
    "        if self.scale:\n",
    "            input = input.div(self.weight.sqrt())\n",
    "\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66055c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseVQHelper:\n",
    "    \"\"\"A context object, which tracks the VQ layers in the module.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        module: nn.Module,\n",
    "        cls: type = PreNormLayer,\n",
    "    ) -> None:\n",
    "        # enumerate the layers and attach our forward hook to them\n",
    "        self._hooks = {}\n",
    "        self._names = {}\n",
    "        self._register(module, cls)\n",
    "        self._enabled = False\n",
    "\n",
    "    def __enter__(self) -> object:\n",
    "        self._enabled = True\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback) -> None:\n",
    "        self._enabled = False\n",
    "\n",
    "    def _register(self, module: nn.Module, cls: type) -> None:\n",
    "        for nom, mod in module.named_modules():\n",
    "            if isinstance(mod, cls) and mod not in self._hooks:\n",
    "                self._hooks[mod] = mod.register_forward_pre_hook(self._hook)\n",
    "                self._names[mod] = nom\n",
    "\n",
    "    def _hook(\n",
    "        self,\n",
    "        module: Module,\n",
    "        inputs: tuple[Tensor, ...],\n",
    "        output: Tensor,\n",
    "    ) -> None:\n",
    "        # don't do anything OUTSIDE the with-scope\n",
    "        if self._enabled:\n",
    "            if module not in self._hooks:\n",
    "                raise RuntimeError\n",
    "\n",
    "            return self.on_forward(module, inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab601fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl = PreNormLayer(0, True, True)\n",
    "input = torch.randn(1024, 8) + 10\n",
    "\n",
    "for x in input.split(8, 0):\n",
    "    pnl._update(x)\n",
    "\n",
    "pnl(x)\n",
    "\n",
    "input.mean(0) - pnl.bias, input.var(0, False) - pnl.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfc29bb",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
