{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4a90ea",
   "metadata": {},
   "source": [
    "# Imitating a good heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec7c79",
   "metadata": {},
   "source": [
    "A simple viz for tracking loss and other runtime series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.il.plotting import plot_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fbc898",
   "metadata": {},
   "source": [
    "SeedSequence needs a `.spawn-one` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng, SeedSequence\n",
    "\n",
    "\n",
    "def spawn_one(ss: SeedSequence) -> SeedSequence:\n",
    "    return ss.spawn(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b39ce",
   "metadata": {},
   "source": [
    "A function to add a dict record to a table (dict-of-lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def do_add(\n",
    "    record: dict, to: dict[..., list], transform: Callable[..., dict] = None\n",
    ") -> dict:\n",
    "    \"\"\"Add the record to a transposed dict of lists.\"\"\"\n",
    "    original = record\n",
    "    if callable(transform):\n",
    "        record = transform(**record)\n",
    "\n",
    "    # assume no fields are missing\n",
    "    for field, value in record.items():\n",
    "        to.setdefault(field, []).append(value)\n",
    "\n",
    "    return original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a24199",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17756684",
   "metadata": {},
   "source": [
    "## Composing the datastreams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7435de2",
   "metadata": {},
   "source": [
    "Reservoir sampling for data from infinite data streams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c28ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def shuffle(\n",
    "    it: Iterable,\n",
    "    n_size: int = 1024,\n",
    "    seed: int = None,\n",
    ") -> Iterable:\n",
    "    \"\"\"Shuffle the values from the iterable\"\"\"\n",
    "\n",
    "    rng, reservoir = default_rng(seed), []\n",
    "    for item in it:\n",
    "        # stack elements until the reservior is full\n",
    "        if len(reservoir) < n_size:\n",
    "            reservoir.append(item)\n",
    "            continue\n",
    "\n",
    "        # replace a random element with the new item\n",
    "        ix = rng.choice(n_size)\n",
    "        yield reservoir[ix]\n",
    "        reservoir[ix] = item\n",
    "\n",
    "    # re-shuffle the remaining items (in the case the sequence\n",
    "    #  was too short for proper mixing)\n",
    "    rng.shuffle(reservoir)\n",
    "    yield from reservoir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3907b9",
   "metadata": {},
   "source": [
    "Batch the data in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(it: Iterable, n_size: int = 16) -> Iterable[list]:\n",
    "    \"\"\"Batch the values from the iterable\"\"\"\n",
    "    batch = []\n",
    "    for item in it:\n",
    "        batch.append(item)\n",
    "\n",
    "        # produce the batch when it's full and then clear\n",
    "        if len(batch) >= n_size:\n",
    "            yield batch\n",
    "            # `.clear` messes up saving batches for reuse\n",
    "            batch = []\n",
    "\n",
    "    # don't forget the residual batch\n",
    "    if batch:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a1863",
   "metadata": {},
   "source": [
    "a handy sequence limiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "def limit(it: Iterable, limiter: Union[int, Iterable]) -> Iterable:\n",
    "    \"\"\"Limit the length of the sequence to at most `n_total` values\"\"\"\n",
    "    limiter = range(limiter) if isinstance(limiter, int) else limiter\n",
    "    for item, _ in zip(it, limiter):\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d757b10",
   "metadata": {},
   "source": [
    "finally, a generator that mixes data from multiple iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixer(*its: Iterable, seed: int = None) -> Iterable:\n",
    "    \"\"\"Yield a value from an iterable picked at random each time\"\"\"\n",
    "    iters = list(map(iter, its))\n",
    "\n",
    "    rng = default_rng(seed)\n",
    "    while iters:\n",
    "        it = rng.choice(iters, shuffle=False)\n",
    "        try:\n",
    "            yield next(it)\n",
    "\n",
    "        except StopIteration:\n",
    "            iters.remove(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13fc53",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddeff38",
   "metadata": {},
   "source": [
    "## CO and Branching data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ffac7a",
   "metadata": {},
   "source": [
    "The following SCIP settings were inherited from [Gasse et al. 2019](), [Parsonson et al. 2022](), and\n",
    "[Scavuzzo et al. 2022]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyscipopt\n",
    "import ecole as ec\n",
    "\n",
    "# from toybnb.scip.ecole.il.env import default_scip_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446ff20",
   "metadata": {},
   "source": [
    "A derived branching env that disables SCIP's presolver\n",
    "- without presolve training becomes much slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491390b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from toybnb.scip.ecole.il.env import BranchingWithoutPresolve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ace826",
   "metadata": {},
   "source": [
    "We want to train a problem-dependent branching heuristic. Strong branching and pseudocost have the best problem dependency -- they have access to its geometry, but are slow. Our goal is to train a neural net that extract geometric info from the parametric representation of a problem at a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5222c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A procedure to seed Ecole's PRNG\n",
    "from toybnb.scip.ecole.il.env import ecole_seed, make_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613387e",
   "metadata": {},
   "source": [
    "We source supervised data from the strong branching heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4accacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.il.brancher import strongbranch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af75118",
   "metadata": {},
   "source": [
    "We also compare to a randombranching expert, although its utility is vague."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5514bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.il.brancher import randombranch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb081e3",
   "metadata": {},
   "source": [
    "And, finally, a function to use the trained machine learning model to pick branching vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.il.brancher import batched_ml_branchrule, ml_branchrule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b700c7c",
   "metadata": {},
   "source": [
    "A branchrule that communicates with a central action server, that attempts to process the requests in batches for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecole.environment import Branching\n",
    "from ecole.core.scip import Model\n",
    "\n",
    "from toybnb.scip.ecole.il.brancher import BranchRule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9eb549",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e79622",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc5f0f",
   "metadata": {},
   "source": [
    "## The data source proper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c52e76",
   "metadata": {},
   "source": [
    "During training we need a generator of observation, action, and reward data from the decision nodes of the BnB tree. During evaluation we only care about the final `nfo` data from the branching environment, as it contains the post-search tree stats. `rollout` returns both kinds of data, `evaluate` is a handy wrapper around `rollout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.il.rollout import rollout, evaluate\n",
    "\n",
    "\n",
    "# We use infinite problem generators, for which we implement a continuous rollout wrapper\n",
    "def continuous_rollout(\n",
    "    it: Iterable[Model], env: Branching, branchrule: BranchRule, kwargs: dict = None\n",
    ") -> Iterable:\n",
    "    # we use for-loop in case an infinite generator is actually finite\n",
    "    for p in it:\n",
    "        yield from rollout(p, env, branchrule, kwargs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "005d0900",
   "metadata": {},
   "source": [
    "co_it = ec.instance.CombinatorialAuctionGenerator()\n",
    "\n",
    "data = []\n",
    "for obs, act, rew in rollout(next(co_it), make_env(), strongbranch(), {}, None):\n",
    "    data.append((obs, act, rew))\n",
    "\n",
    "evaluate(next(co_it), make_env(), strongbranch())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d1c03",
   "metadata": {},
   "source": [
    "A multithreaded version for especially slow branchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce48cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.il.rollout import pool_rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1979052",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141b17b",
   "metadata": {},
   "source": [
    "## the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac1a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import Module, functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782cb876",
   "metadata": {},
   "source": [
    "A tracer hook to help with debugging live models\n",
    "\n",
    "```python\n",
    "hook = model.register_forward_pre_hook(tracer)\n",
    "\n",
    "bt, by = next(feed)\n",
    "model(bt, by)\n",
    "\n",
    "...\n",
    "\n",
    "hook.remove()\n",
    "```\n",
    "- _judicious placement_ of breakpoints `b` is advised so that continuation `c` would work\n",
    "    - try `n`, `b 1208`\n",
    "- otherwise use `n` for next, `s` for step inside, `u/d` to move between stack frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracer(module, input) -> None:\n",
    "    import pdb\n",
    "\n",
    "    pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def71b8",
   "metadata": {},
   "source": [
    "A good old trusted MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.il.nn import mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1663a59",
   "metadata": {},
   "source": [
    "The original message passing architecture is\n",
    "$$\n",
    "x^k_i\n",
    "    = \\gamma\\bigl(\n",
    "        x^{k-1}_i,\n",
    "        \\operatorname{\\diamond} \\bigl(\n",
    "            \\bigl\\{\n",
    "                \\phi(x^{k-1}_i, x^{k-1}_j, e_{ij})\n",
    "                \\colon j \\in G_i\n",
    "            \\bigr\\}\n",
    "        \\bigr)\n",
    "    \\bigr)\n",
    "    \\,, $$\n",
    "\n",
    "where $\\diamond$ is a permutation-invariant set-to-real _aggregation_ operator,\n",
    "$\\phi$ is the _message_ function, and $\\gamma$ is the _output_ function, and $G_u$\n",
    "is the graph neighborhood of the vertex $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97994190",
   "metadata": {},
   "source": [
    "Below we implement a variant of the massage passing where the _aggregation_ and _message_ operations are based on multi-headed cross attention. This means, that $\\diamond$ and $\\phi$ are _fused_:\n",
    "$$\n",
    "    (\\diamond \\circ \\phi)\n",
    "    \\colon \\mathbb{R}^d \\times \\bigl(\\mathbb{R}^d\\bigr)^* \\to \\mathbb{R}^d\n",
    "    \\colon (x_i, \\{x_j\\colon j \\in G_i\\}) \\mapsto\n",
    "        \\sum_{j \\in G_i} p_j \\phi^v(x_j)\n",
    "    \\,. $$\n",
    "with $\\log p_j \\propto \\phi^q(x_i)^\\top \\phi^k(x_j)$ for $j \\in G_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f31923",
   "metadata": {},
   "source": [
    "Implement a special layer that computes message passing with cross-attention between the parts of a bipartite graph. We make heavy use of `torch-scatter` operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1347483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "from einops import rearrange\n",
    "from torch_scatter import scatter_softmax, scatter_sum\n",
    "\n",
    "# from toybnb.scip.ecole.il.nn import BipartiteMHXA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70273f92",
   "metadata": {},
   "source": [
    "A bipartite transformer block\n",
    "* we opt for post-norm architecture\n",
    "* [Zhang et al. (2022)](https://arxiv.org/abs/2206.11925.pdf) metnion that norm-first layer norm may impact expressivty, but do not analyze when the norm is at the other end\n",
    "* while [](https://arxiv.org/abs/2002.04745.pdf) claim that pre-LN stabilizes the grad, whereas post-LN expected grads' magnitude grows with the depth\n",
    "  - strange, but this appears to contradict their own theoreical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe48bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from toybnb.scip.ecole.il.nn import BipartiteBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce1711",
   "metadata": {},
   "source": [
    "The full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_log_softmax, scatter_logsumexp\n",
    "from torch_scatter import scatter_max, scatter_mean\n",
    "\n",
    "from toybnb.scip.ecole.il.nn import NeuralVariableSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5962ccd4",
   "metadata": {},
   "source": [
    "The network from Gasse et al. 2019 with PreNorm layers replaced by batchnorms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9965e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from toybnb.scip.ecole.il.data import BatchObservation\n",
    "from toybnb.scip.ecole.il.nn import BipartiteGConv, NeuralClassifierBranchruleMixin\n",
    "\n",
    "\n",
    "class Gasse2019(Module, NeuralClassifierBranchruleMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dim_vars: int = 19,\n",
    "        n_dim_cons: int = 5,\n",
    "        n_embed: int = 64,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleDict(\n",
    "            dict(\n",
    "                vars=nn.Sequential(\n",
    "                    # PreNormLayer,\n",
    "                    # nn.BatchNorm1d(n_dim_vars, affine=False),\n",
    "                    nn.Linear(n_dim_vars, n_embed),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_embed, n_embed),\n",
    "                    nn.ReLU(),\n",
    "                ),\n",
    "                cons=nn.Sequential(\n",
    "                    # PreNormLayer,\n",
    "                    # nn.BatchNorm1d(n_dim_cons, affine=False),\n",
    "                    nn.Linear(n_dim_cons, n_embed),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_embed, n_embed),\n",
    "                    nn.ReLU(),\n",
    "                ),\n",
    "                edge=nn.Sequential(\n",
    "                    # PreNormLayer,\n",
    "                    nn.Unflatten(-1, (-1, 1)),\n",
    "                    # nn.BatchNorm1d(1, affine=False),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.v2c = BipartiteGConv(n_embed, True)\n",
    "        self.c2v = BipartiteGConv(n_embed, True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input: BatchObservation) -> Tensor:\n",
    "        jc, jv = input.ctov_ij\n",
    "\n",
    "        # encode the vars and cons features\n",
    "        cons = self.encoder.cons(input.cons)\n",
    "        edge = self.encoder.edge(input.ctov_v)\n",
    "        vars = self.encoder.vars(input.vars)\n",
    "\n",
    "        # bipartite vcv ladder\n",
    "        # (v_{t-1}, c_{t-1}) -->> (v_{t-1}, c_t) -->> (v_t, c_t)\n",
    "        cons = self.c2v(cons, vars, (jc, jv), edge)\n",
    "        vars = self.v2c(vars, cons, (jv, jc), edge)\n",
    "\n",
    "        # get the raw-logit scores of each variable\n",
    "        return self.head(vars).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce3f4da",
   "metadata": {},
   "source": [
    "Some procs for handling dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99866c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_dict(dict: dict[..., dict]) -> dict[..., dict]:\n",
    "    outer = {}\n",
    "    for k_out, inner in dict.items():\n",
    "        for k_in, value in inner.items():\n",
    "            new = outer.setdefault(k_in, {})\n",
    "            assert k_out not in new\n",
    "            new[k_out] = value\n",
    "\n",
    "    return outer\n",
    "\n",
    "\n",
    "def collate_dict(records: list[dict]) -> dict[..., list]:\n",
    "    \"\"\"Collate records assuming no fields are missing\"\"\"\n",
    "    out = {}\n",
    "    for record in records:\n",
    "        for field, value in record.items():\n",
    "            out.setdefault(field, []).append(value)\n",
    "\n",
    "    return {k: np.array(v) for k, v in out.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee12112",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd57a29",
   "metadata": {},
   "source": [
    "## Trainnig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1652ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c5c51",
   "metadata": {},
   "source": [
    "Allow for 100k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d323ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total, n_batch_size = 25_000, 16  # XXX 100k is too long\n",
    "n_reservoir = 512  # XXX 128 reservoir was ok for CAuc\n",
    "\n",
    "C_neg_actset = 0.0  # 1e-3  # XXX the orginal CAuc used to have 0.0\n",
    "C_entropy = 0.0  # 1e-2  # XXX was set to zero in the first CAuc\n",
    "\n",
    "# use a seed sequence with a fixed entropy pool\n",
    "ss = SeedSequence(83278314352113072500167414370310027453)\n",
    "# ss = SeedSequence(None)  # use `ss.entropy` for future reproducibility\n",
    "\n",
    "s_project_graph = \"pre\"  # \"post\"\n",
    "\n",
    "p_drop = 0.2\n",
    "n_embed, n_heads, n_blocks = 64, 4, 1\n",
    "b_edges = True\n",
    "b_norm_first = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe36b8f",
   "metadata": {},
   "source": [
    "Pipe the generator that mixes several the CO problems into the continupus rollout iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.benchmarks import gasse2019\n",
    "from ecole.instance import CombinatorialAuctionGenerator\n",
    "\n",
    "\n",
    "# init the branching env\n",
    "env = make_env(spawn_one(ss))\n",
    "\n",
    "# CAuc(100, 500), CAuc(50, 250)\n",
    "gens = [\n",
    "    CombinatorialAuctionGenerator(100, 500, rng=ecole_seed(spawn_one(ss))),\n",
    "    CombinatorialAuctionGenerator(50, 250, rng=ecole_seed(spawn_one(ss))),\n",
    "]\n",
    "\n",
    "# Use co problems from Gasse 2019\n",
    "# gens = transpose_dict(gasse2019(spawn_one(ss)))\n",
    "# gens = gens[\"train\"].values()\n",
    "itco = mixer(*gens, seed=spawn_one(ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c4d22",
   "metadata": {},
   "source": [
    "set up the rollout observation feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "n_jobs = 12\n",
    "feed = pool_rollout(\n",
    "    itco,\n",
    "    [partial(make_env, fork) for fork in ss.spawn(n_jobs)],\n",
    "    strongbranch(False),\n",
    "    maxsize=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0734367",
   "metadata": {},
   "source": [
    "Then feed the branching observation data into a shuffler, limiter and then batcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f6118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "# set up the data stream\n",
    "# XXX if we put a limiter on the source iter, then shuffle's reservoir\n",
    "#  would consume `n_reservoir`, which are never going to be yield,\n",
    "#  if the source is infinite. In this case, instead, we should put\n",
    "#  a limiter on the `shuffle` itself.\n",
    "it = feed\n",
    "it = limit(it, trange(n_total, ncols=70))\n",
    "it = shuffle(it, n_reservoir, spawn_one(ss))\n",
    "it = batch(it, n_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858e728",
   "metadata": {},
   "source": [
    "Set up the model and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import SequentialLR\n",
    "from torch.optim.lr_scheduler import LinearLR, ConstantLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "mod = NeuralVariableSelector(\n",
    "    19,\n",
    "    5,\n",
    "    n_embed,\n",
    "    n_heads,\n",
    "    n_blocks,\n",
    "    p_drop,\n",
    "    b_norm_first=b_norm_first,\n",
    "    s_project_graph=s_project_graph,\n",
    "    b_edges=b_edges,\n",
    ")\n",
    "\n",
    "# mod = Gasse2019()\n",
    "mod.to(device)\n",
    "\n",
    "# optim = torch.optim.AdamW(mod.parameters(), lr=3e-4)\n",
    "optim = torch.optim.AdamW(mod.parameters(), lr=1e-3)\n",
    "\n",
    "sched = None\n",
    "sched = SequentialLR(\n",
    "    optim,\n",
    "    [\n",
    "        ConstantLR(optim, factor=0.5, total_iters=50),\n",
    "        LinearLR(optim, start_factor=0.5, total_iters=250),\n",
    "        CosineAnnealingLR(optim, T_max=250, eta_min=1e-6),\n",
    "    ],\n",
    "    [50, 300],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c425f",
   "metadata": {},
   "source": [
    "train an IL model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24f0c1b4",
   "metadata": {},
   "source": [
    "hk = mod.register_forward_pre_hook(tracer)\n",
    "hk.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f1171",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch import as_tensor\n",
    "from toybnb.scip.ecole.il.data import collate\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "log = dict()\n",
    "for bt in it:\n",
    "    # collate the batch\n",
    "    obs, act, rew = zip(*bt)\n",
    "    bx = collate(obs, device=device)\n",
    "    act = as_tensor(np.array(act, dtype=int), device=device)\n",
    "    rew = np.array(rew, dtype=np.float32)\n",
    "\n",
    "    # forward pass\n",
    "    mod.train()\n",
    "    _, terms = mod.compute(bx, target=act)\n",
    "    terms = {k: v.mean() for k, v in terms.items()}\n",
    "\n",
    "    # log likelihoods and entropy have the same uints (and thus scale)\n",
    "    loss = (\n",
    "        # (min) -ve log-likelihood\n",
    "        terms[\"neg_target\"]\n",
    "        # (max) entropy\n",
    "        - C_entropy * terms[\"entropy\"]\n",
    "        # (max) -ve log-likelihood of forbidden actions\n",
    "        - C_neg_actset * terms[\"neg_actset\"]\n",
    "    )\n",
    "\n",
    "    # backprop\n",
    "    mod.zero_grad(True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if sched is not None:\n",
    "        sched.step()\n",
    "\n",
    "    mod.eval()\n",
    "    do_add({k: float(v) for k, v in terms.items()}, log)\n",
    "\n",
    "    clear_output(True)\n",
    "    fig, ax0 = plt.subplots(1, 1, figsize=(5, 2), dpi=200, sharex=True)\n",
    "    plot_series(ax0, **log)\n",
    "    ax0.set_title(\"terms\")\n",
    "    ax0.legend(loc=\"lower left\", fontsize=\"xx-small\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41875b1c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime\n",
    "\n",
    "torch.save(\n",
    "    dict(\n",
    "        __dttm__=strftime(\"%Y%m%d-%H%M%S\"),\n",
    "        state_dict=mod.state_dict(),\n",
    "    ),\n",
    "    \"dump/cauc-norm-first.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08278268",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
