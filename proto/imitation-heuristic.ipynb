{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4a90ea",
   "metadata": {},
   "source": [
    "# A dqn agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7505b82",
   "metadata": {},
   "source": [
    "The pp-curve drawing procedure. We compare distibutions using the pp-curve, which is analogous to the ROC curve: pp compares two independent distributions, while ROC compares the true-, false- positive distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "\n",
    "\n",
    "def pp_curve(*, x: ndarray, y: ndarray, num: int = None) -> tuple[ndarray, ndarray]:\n",
    "    \"\"\"Build threshold-parameterized pipi curve.\"\"\"\n",
    "    # sort each sample for fast O(\\log n) eCDF queries by `searchsorted`\n",
    "    x, y = np.sort(x), np.sort(y)\n",
    "\n",
    "    # pool sorted samples to get thresholds\n",
    "    xy = np.concatenate((x, y))\n",
    "    if num is None:\n",
    "        # finest detail thresholds: sort the pooled samples (sorted\n",
    "        #  arrays can be merged in O(n), but it turns out numpy does\n",
    "        #  not have the procedure)\n",
    "        xy.sort()\n",
    "\n",
    "    else:\n",
    "        # coarsen by finding threshold grid in the pooled sample, that\n",
    "        #  is equispaced after being transformed by the empirical cdf.\n",
    "        xy = np.quantile(xy, np.linspace(0, 1, num=num), method=\"linear\")\n",
    "\n",
    "    # add +ve/-ve inf end points to the parameter value sequence\n",
    "    xy = np.r_[-np.inf, xy, +np.inf]\n",
    "\n",
    "    # we build the pp-curve the same way as we build the ROC curve:\n",
    "    #  by parameterizing with the a monotonic threshold sequence\n",
    "    #    pp: v \\mapsto (\\hat{F}_x(v), \\hat{F}_y(v))\n",
    "    #  where \\hat{F}_S(v) = \\frac1{n_S} \\sum_j 1_{S_j \\leq v}\n",
    "    p = np.searchsorted(x, xy) / len(x)\n",
    "    q = np.searchsorted(y, xy) / len(y)\n",
    "\n",
    "    return p, q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec7c79",
   "metadata": {},
   "source": [
    "A simple viz for tracking loss and other runtime series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.axes import Axes\n",
    "\n",
    "\n",
    "def plot_stats(ax: Axes = None, n_last: int = 25, **series) -> None:\n",
    "    ax = plt.gca() if ax is None else ax\n",
    "\n",
    "    els = {}\n",
    "    for name, x in series.items():\n",
    "        (el,) = ax.plot(x, label=name)\n",
    "        # add the average estimate tick to the right-hand side\n",
    "        # XXX throws a warning on all-nan slices\n",
    "        avg, col = np.nanmean(x[-n_last:]), el.get_color()\n",
    "        ax.axhline(\n",
    "            avg,\n",
    "            0.975,\n",
    "            c=col,\n",
    "            alpha=0.25,\n",
    "            zorder=-10,\n",
    "        )\n",
    "        ax.annotate(\n",
    "            f\"{avg:.2g}\",\n",
    "            c=col,\n",
    "            fontsize=\"xx-small\",\n",
    "            xy=(1.005, avg),\n",
    "            xytext=(0.0, -2.0),\n",
    "            xycoords=(\"axes fraction\", \"data\"),\n",
    "            textcoords=\"offset points\",\n",
    "            zorder=-10,\n",
    "        )\n",
    "        els[name] = el\n",
    "\n",
    "    ax.legend(els.values(), series, loc=\"best\", fontsize=\"x-small\", ncol=3)\n",
    "\n",
    "    return els"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fbc898",
   "metadata": {},
   "source": [
    "SeedSequence needs a `.spawn-one` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng, SeedSequence\n",
    "\n",
    "\n",
    "def spawn_one(ss: SeedSequence) -> SeedSequence:\n",
    "    return ss.spawn(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b39ce",
   "metadata": {},
   "source": [
    "A function to add a dict record to a table (dict-of-lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def do_add(\n",
    "    record: dict, to: dict[..., list], transform: Callable[..., dict] = None\n",
    ") -> dict:\n",
    "    \"\"\"Add the record to a transposed dict of lists.\"\"\"\n",
    "    original = record\n",
    "    if callable(transform):\n",
    "        record = transform(**record)\n",
    "\n",
    "    # assume no fields are missing\n",
    "    for field, value in record.items():\n",
    "        to.setdefault(field, []).append(value)\n",
    "\n",
    "    return original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a24199",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17756684",
   "metadata": {},
   "source": [
    "## Composing the datastreams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7435de2",
   "metadata": {},
   "source": [
    "Reservoir sampling for data from infinite data streams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c28ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def shuffle(\n",
    "    it: Iterable,\n",
    "    n_size: int = 1024,\n",
    "    seed: int = None,\n",
    ") -> Iterable:\n",
    "    \"\"\"Shuffle the values from the iterable\"\"\"\n",
    "\n",
    "    rng, reservoir = default_rng(seed), []\n",
    "    for sample in it:\n",
    "        # stack elements until the reservior is full\n",
    "        if len(reservoir) < n_size:\n",
    "            reservoir.append(sample)\n",
    "            continue\n",
    "\n",
    "        # replace a random element with the new sample\n",
    "        ix = rng.choice(n_size)\n",
    "        yield reservoir[ix]\n",
    "        reservoir[ix] = sample\n",
    "\n",
    "    # re-shuffle the remaining samples (in the case\n",
    "    #  the sequence was too short for proper mixing)\n",
    "    rng.shuffle(reservoir)\n",
    "    yield from reservoir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3907b9",
   "metadata": {},
   "source": [
    "Batch the data in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(it: Iterable, n_size: int = 16) -> Iterable[list]:\n",
    "    \"\"\"Batch the values from the iterable\"\"\"\n",
    "    batch = []\n",
    "    for sample in it:\n",
    "        batch.append(sample)\n",
    "\n",
    "        # produce the batch when it's full and then clear\n",
    "        if len(batch) >= n_size:\n",
    "            yield batch\n",
    "            batch.clear()\n",
    "\n",
    "    # don't forget the residual batch\n",
    "    if batch:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a1863",
   "metadata": {},
   "source": [
    "a handy sequence limiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "def limit(it: Iterable, limiter: Union[int, Iterable]) -> Iterable:\n",
    "    \"\"\"Limit the length of the sequence to at most `n_total` values\"\"\"\n",
    "    limiter = range(limiter) if isinstance(limiter, int) else limiter\n",
    "    for sample, _ in zip(it, limiter):\n",
    "        yield sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d757b10",
   "metadata": {},
   "source": [
    "finally, a generator that mixes data from multiple iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixer(*its: Iterable, seed: int = None) -> Iterable:\n",
    "    \"\"\"Yield a value from an iterable picked at random each time\"\"\"\n",
    "    iters = list(map(iter, its))\n",
    "\n",
    "    rng = default_rng(seed)\n",
    "    while iters:\n",
    "        it = rng.choice(iters, shuffle=False)\n",
    "        try:\n",
    "            yield next(it)\n",
    "\n",
    "        except StopIteration:\n",
    "            iters.remove(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13fc53",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddeff38",
   "metadata": {},
   "source": [
    "## CO and Branching data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ffac7a",
   "metadata": {},
   "source": [
    "The following SCIP settings were inherited from [Gasse et al. 2019](), [Parsonson et al. 2022](), and\n",
    "[Scavuzzo et al. 2022]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyscipopt\n",
    "import ecole as ec\n",
    "\n",
    "\n",
    "def default_scip_params() -> dict:\n",
    "    #\n",
    "    return {\n",
    "        # although we use ecole's `.disable_presolve()`, we still keep these params\n",
    "        \"separating/maxrounds\": 0,  # separate (cut) only at root node\n",
    "        \"presolving/maxrestarts\": 0,  # disable solver restarts\n",
    "        # determines scip's inner clock and affects the time limit\n",
    "        \"timing/clocktype\": 1,  # 1: CPU user seconds, 2: wall clock time\n",
    "        \"limits/time\": 60 * 60,  # solver time limit\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446ff20",
   "metadata": {},
   "source": [
    "A derived branching env that disables SCIP's presolver\n",
    "- without presolve training becomes much slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491390b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecole.environment import Branching\n",
    "\n",
    "\n",
    "class BranchingWithoutPresolve:\n",
    "    def reset(self, instance, *dynamics_args, **dynamics_kwargs):\n",
    "        # disable presolve through ecole\n",
    "        # XXX [.disable_presolve](./libecole/src/scip/model.cpp#L195)\n",
    "        #  calls [SCIPsetPresolving](./src/scip/scip_params.c#L913-937)\n",
    "        instance = instance.copy_orig()\n",
    "        instance.disable_presolve()\n",
    "\n",
    "        return super().reset(instance, *dynamics_args, **dynamics_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d937e1ac",
   "metadata": {},
   "source": [
    "SCIP intercepts the keyboard interrput, making it impossible to abort a seemingly stuck loop. We intercept the signal and delay it until the endo fo the `with` scope.\n",
    "\n",
    "* for `CAuc(100, 500)` the bipartite obs takes up about `260kb`, which means that precomputin 100k samples is out of question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "\n",
    "class DelaySIGINT:\n",
    "    def __init__(self, disable: bool = False) -> None:\n",
    "        self.disable = disable\n",
    "\n",
    "    def __bool__(self):\n",
    "        return self.signal is not None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.signal = None\n",
    "        if self.disable:\n",
    "            return self\n",
    "\n",
    "        self.old_handler = signal.getsignal(signal.SIGINT)\n",
    "        signal.signal(signal.SIGINT, self.handler)\n",
    "        return self\n",
    "\n",
    "    def handler(self, sig, frame):\n",
    "        self.signal = sig, frame\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        if not self.disable:\n",
    "            signal.signal(signal.SIGINT, self.old_handler)\n",
    "            if self.signal is not None:\n",
    "                self.old_handler(*self.signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ace826",
   "metadata": {},
   "source": [
    "We want to train a problem-dependent branching heuristic. Strong branching and pseudocost have the best problem dependency -- they have access to its geometry, but are slow. Our goal is to train a neural net that extract geometric info from the parametric representation of a problem at a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5222c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecole.observation import NodeBipartite\n",
    "from ecole.reward import Constant\n",
    "\n",
    "\n",
    "def make_env(\n",
    "    entropy: int = None, presolve: bool = True, scip_params: dict = np._NoValue\n",
    ") -> Branching:\n",
    "    # fork the seed sequence from the given entropy\n",
    "    ss = entropy if isinstance(entropy, SeedSequence) else SeedSequence(entropy)\n",
    "\n",
    "    # allow for `true None` scip-params\n",
    "    if scip_params is np._NoValue:\n",
    "        scip_params = default_scip_params()\n",
    "\n",
    "    # choose the env\n",
    "    cls = Branching if presolve else BranchingWithoutPresolve\n",
    "\n",
    "    # the branching env\n",
    "    env = cls(\n",
    "        # We use bipartite graph repr of the node's LP\n",
    "        observation_function=NodeBipartite(),\n",
    "        # No reward function at, since we imitate an expert\n",
    "        # reward_function=ec.reward.PrimalDualIntegral(),\n",
    "        reward_function=Constant(float(\"nan\")),\n",
    "        # we track the aggregate tree stats\n",
    "        # XXX not sure if nnodes is a `clean` metric\n",
    "        information_function={\n",
    "            \"n_nodes\": ec.reward.NNodes().cumsum(),\n",
    "            \"n_lpiter\": ec.reward.LpIterations().cumsum(),\n",
    "            \"f_soltime\": ec.reward.SolvingTime().cumsum(),\n",
    "            # 'primal_integral': ec.reward.PrimalIntegral().cumsum(),\n",
    "            # 'dual_integral': ec.reward.DualIntegral().cumsum(),\n",
    "            # 'primal_dual_integral': ec.reward.PrimalDualIntegral(),\n",
    "        },\n",
    "        scip_params=scip_params,\n",
    "    )\n",
    "\n",
    "    # `RandomGenerator.max_seed` reports 2^{32}-1\n",
    "    (seed,) = ss.generate_state(1, dtype=np.uint32)\n",
    "    env.seed(int(seed))\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dce7df9",
   "metadata": {},
   "source": [
    "We use a special representation of a batch of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from ecole.observation import NodeBipartiteObs\n",
    "from torch import Tensor\n",
    "from numpy import ndarray\n",
    "\n",
    "\n",
    "class Observation(NamedTuple):\n",
    "    obs: NodeBipartiteObs\n",
    "    actset: ndarray\n",
    "\n",
    "\n",
    "class BatchObservation(NamedTuple):\n",
    "    # variable, constraint, and cons-to-vars link features\n",
    "    vars: Tensor\n",
    "    cons: Tensor\n",
    "\n",
    "    ctov_v: Tensor\n",
    "    ctov_ij: Tensor\n",
    "\n",
    "    actset: Tensor\n",
    "\n",
    "    # the first index in the collated batch of the span of data\n",
    "    #  originating from each uncolated batch element\n",
    "    ptr_vars: Tensor\n",
    "    ptr_cons: Tensor\n",
    "    ptr_ctov: Tensor\n",
    "    ptr_actset: Tensor\n",
    "\n",
    "    # batch affinity\n",
    "    inx_vars: Tensor\n",
    "    inx_cons: Tensor\n",
    "    inx_ctov: Tensor\n",
    "    inx_actset: Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6b09c",
   "metadata": {},
   "source": [
    "A special collation logic for `Observation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import as_tensor\n",
    "\n",
    "\n",
    "def collate(batch: tuple[Observation], device: torch.device = None) -> BatchObservation:\n",
    "    \"\"\"Collate `NodeBipartiteObs` into torch tensors\"\"\"\n",
    "    obs, actset = zip(*batch)\n",
    "\n",
    "    # prepare vars\n",
    "    vars = [x.variable_features for x in obs]\n",
    "    n_vars = sum(map(len, vars))\n",
    "    x_vars = torch.empty(\n",
    "        (n_vars, *vars[0].shape[1:]),\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    ptr_vars = x_vars.new_zeros(1 + len(vars), dtype=torch.long)\n",
    "    inx_vars = x_vars.new_empty(n_vars, dtype=torch.long)\n",
    "\n",
    "    # prepare cons\n",
    "    cons = [x.row_features for x in obs]\n",
    "    n_cons = sum(map(len, cons))\n",
    "    x_cons = x_vars.new_empty((n_cons,) + cons[0].shape[1:])\n",
    "\n",
    "    ptr_cons = x_cons.new_zeros(1 + len(cons), dtype=torch.long)\n",
    "    inx_cons = x_cons.new_empty(n_cons, dtype=torch.long)\n",
    "\n",
    "    # prepare edges (coo ijv, cons-to-vars)\n",
    "    ctov = [x.edge_features for x in obs]\n",
    "    n_ctov = sum(len(e.values) for e in ctov)\n",
    "    x_ctov_v = x_vars.new_empty(n_ctov)\n",
    "    x_ctov_ij = x_vars.new_empty((2, n_ctov), dtype=torch.long)\n",
    "\n",
    "    ptr_ctov = x_ctov_v.new_zeros(1 + len(ctov), dtype=torch.long)\n",
    "    inx_ctov = x_ctov_v.new_empty(n_ctov, dtype=torch.long)\n",
    "\n",
    "    # prepare the collated action set\n",
    "    n_actset = sum(map(len, actset))\n",
    "    x_actset = x_vars.new_empty(n_actset, dtype=torch.long)\n",
    "\n",
    "    ptr_actset = x_actset.new_zeros(1 + len(actset), dtype=torch.long)\n",
    "    inx_actset = x_actset.new_empty(n_actset, dtype=torch.long)\n",
    "\n",
    "    # copy numpy data into the allocated tensors\n",
    "    v1 = c1 = e1 = j1 = 0\n",
    "    for b, (x, act_set) in enumerate(batch):\n",
    "        v0, v1 = v1, v1 + len(x.variable_features)\n",
    "        c0, c1 = c1, c1 + len(x.row_features)\n",
    "        e0, e1 = e1, e1 + len(x.edge_features.values)\n",
    "        j0, j1 = j1, j1 + len(act_set)\n",
    "\n",
    "        # the vars, cons, and cons-to-vars (edges)\n",
    "        x_vars[v0:v1].copy_(as_tensor(x.variable_features))\n",
    "        x_cons[c0:c1].copy_(as_tensor(x.row_features))\n",
    "        x_ctov_v[e0:e1].copy_(as_tensor(x.edge_features.values))\n",
    "        x_ctov_ij[:, e0:e1].copy_(as_tensor(x.edge_features.indices.astype(int)))\n",
    "        x_actset[j0:j1].copy_(as_tensor(act_set.astype(int)))\n",
    "\n",
    "        # fixup the ij-link indices and action set\n",
    "        x_ctov_ij[0, e0:e1] += c0\n",
    "        x_ctov_ij[1, e0:e1] += v0\n",
    "        x_actset[j0:j1] += v0\n",
    "\n",
    "        # the batch assignment\n",
    "        inx_vars[v0:v1] = b\n",
    "        inx_cons[c0:c1] = b\n",
    "        inx_ctov[e0:e1] = b\n",
    "        inx_actset[j0:j1] = b\n",
    "\n",
    "        # record the batch index pointer\n",
    "        ptr_vars[1 + b] = v1\n",
    "        ptr_cons[1 + b] = c1\n",
    "        ptr_ctov[1 + b] = e1\n",
    "        ptr_actset[1 + b] = j1\n",
    "\n",
    "    return BatchObservation(\n",
    "        x_vars,\n",
    "        x_cons,\n",
    "        x_ctov_v,\n",
    "        x_ctov_ij,\n",
    "        x_actset,\n",
    "        ptr_vars,\n",
    "        ptr_cons,\n",
    "        ptr_ctov,\n",
    "        ptr_actset,\n",
    "        inx_vars,\n",
    "        inx_cons,\n",
    "        inx_ctov,\n",
    "        inx_actset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613387e",
   "metadata": {},
   "source": [
    "We source supervised data from the strong branching heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4accacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecole.observation import StrongBranchingScores, Pseudocosts\n",
    "from ecole.core.scip import Model, Stage\n",
    "\n",
    "BranchRule = Callable[[Branching], int]\n",
    "BranchRuleCallable = Callable[[Observation], int]\n",
    "\n",
    "\n",
    "def strongbranch(pseudocost: bool = False) -> BranchRule:\n",
    "    if not pseudocost:\n",
    "        scorer = StrongBranchingScores(pseudo_candidates=False)\n",
    "\n",
    "    else:\n",
    "        scorer = Pseudocosts()\n",
    "\n",
    "    def _spawn(env: Branching) -> BranchRuleCallable:\n",
    "        def _branchrule(obs: Observation, **ignored) -> int:\n",
    "            if env.model.stage != Stage.Solving:\n",
    "                return None\n",
    "\n",
    "            scores = scorer.extract(env.model, False)\n",
    "            return obs.actset[scores[obs.actset].argmax()]  # SCIPvarGetProbindex\n",
    "\n",
    "        return _branchrule\n",
    "\n",
    "    return _spawn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af75118",
   "metadata": {},
   "source": [
    "We also compare to a randombranching expert, although its utility is vague."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5514bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randombranch(seed: int = None) -> BranchRule:\n",
    "    rng = default_rng(seed)\n",
    "\n",
    "    def _spawn(env: Branching) -> BranchRuleCallable:\n",
    "        def _branchrule(obs: Observation, **ignored) -> int:\n",
    "            if env.model.stage == Stage.Solving:\n",
    "                return int(rng.choice(obs.actset))\n",
    "            return None\n",
    "\n",
    "        return _branchrule\n",
    "\n",
    "    return _spawn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb081e3",
   "metadata": {},
   "source": [
    "And, finally, a function to use the trained machine learning model to pick branching vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "def batched_ml_branchrule(module: Module) -> BranchRuleCallable:\n",
    "    def _branchrule(batch: tuple[Observation], **ignored) -> tuple[int]:\n",
    "        module.eval()\n",
    "        out = module.predict(collate(batch)).cpu()\n",
    "        return np.asarray(out, dtype=int).tolist()\n",
    "\n",
    "    return torch.inference_mode(True)(_branchrule)\n",
    "\n",
    "\n",
    "def ml_branchrule(module: Module) -> BranchRule:\n",
    "    do_batch = batched_ml_branchrule(module)\n",
    "\n",
    "    def _spawn(env: Branching) -> BranchRuleCallable:\n",
    "        def _branchrule(obs: Observation, **ignored) -> int:\n",
    "            if env.model.stage != Stage.Solving:\n",
    "                return None\n",
    "\n",
    "            # apply the model to a single-item batch\n",
    "            return int(do_batch([obs])[0])\n",
    "\n",
    "        return _branchrule\n",
    "\n",
    "    return _spawn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b700c7c",
   "metadata": {},
   "source": [
    "A server that attempts to batch-process the requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread, Event\n",
    "from queue import Queue, Empty as QueueEmpty\n",
    "\n",
    "\n",
    "class BatchProcessor(Thread):\n",
    "    \"\"\"Collect requests and batch process them with target\"\"\"\n",
    "\n",
    "    timeout: float = 3.0\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: Callable,\n",
    "        name: str = None,\n",
    "        daemon: bool = None,\n",
    "    ) -> None:\n",
    "        super().__init__(name=name, daemon=daemon)\n",
    "        self.exception_, self.target = None, target\n",
    "        self.is_finished, self.requests = Event(), Queue()\n",
    "\n",
    "    def stop(self) -> None:\n",
    "        self.is_finished.set()\n",
    "        self.join()\n",
    "\n",
    "    def run(self) -> None:\n",
    "        batch = []\n",
    "        while not self.is_finished.is_set():\n",
    "            # collect the first element by a short-lived blocking call\n",
    "            try:\n",
    "                batch.append(self.requests.get(True, timeout=self.timeout))\n",
    "\n",
    "            except QueueEmpty:\n",
    "                continue\n",
    "\n",
    "            # fetch all __immediately__ available items\n",
    "            try:\n",
    "                while True:\n",
    "                    batch.append(self.requests.get(False, timeout=None))\n",
    "\n",
    "            except QueueEmpty:\n",
    "                pass\n",
    "\n",
    "            # separate the data from the tx queue\n",
    "            coms, inputs = zip(*batch)\n",
    "            batch.clear()\n",
    "\n",
    "            # process and send each result back to its origin,\n",
    "            # but auto-shutdown in case of emergency\n",
    "            try:\n",
    "                for com, out in zip(coms, self.target(inputs)):\n",
    "                    com.put(out)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.exception_ = e\n",
    "                break\n",
    "\n",
    "        self.is_finished.set()\n",
    "\n",
    "    def connect(self) -> Callable:\n",
    "        \"\"\"create a communications closure\"\"\"\n",
    "        com = Queue()\n",
    "\n",
    "        def co_yield(input: ...) -> ...:\n",
    "            # send the request, unless the servser has been terminated\n",
    "            if self.is_finished.is_set():\n",
    "                raise RuntimeError\n",
    "\n",
    "            self.requests.put((com, input))\n",
    "\n",
    "            # wait on the exclusive queue, making sure not to block\n",
    "            #  for too long\n",
    "            while not self.is_finished.is_set():\n",
    "                try:\n",
    "                    return com.get(True, timeout=self.timeout)\n",
    "\n",
    "                except QueueEmpty:\n",
    "                    continue\n",
    "\n",
    "            raise self.exception_ or RuntimeError\n",
    "\n",
    "        return co_yield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fcf1a9",
   "metadata": {},
   "source": [
    "A batching branchrule server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25229a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BranchingServer(BatchProcessor):\n",
    "    \"\"\"Branching variable Server\"\"\"\n",
    "\n",
    "    def connect(self, env: Branching) -> BranchRuleCallable:\n",
    "        \"\"\"Spawn a new branchrule\"\"\"\n",
    "        co_yield = super().connect()\n",
    "\n",
    "        def _branchrule(obs: Observation, **ignored: dict) -> int:\n",
    "            if env.model.stage != Stage.Solving:\n",
    "                return None\n",
    "\n",
    "            return int(co_yield(obs))\n",
    "\n",
    "        return _branchrule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2d86e",
   "metadata": {},
   "source": [
    "A procedure to seed Ecole's PRNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d48a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecole import RandomGenerator\n",
    "\n",
    "\n",
    "def ecole_seed(ss: SeedSequence) -> RandomGenerator:\n",
    "    # `RandomGenerator.max_seed` reports 2^{32}-1\n",
    "    (seed,) = ss.generate_state(1, dtype=np.uint32)\n",
    "    return RandomGenerator(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e79622",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc5f0f",
   "metadata": {},
   "source": [
    "## The data source proper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c52e76",
   "metadata": {},
   "source": [
    "A generator of observation-action-reward data collected from the nodes of SCIP's BnB search tree at which a branching decision was made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d23fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "from ecole.scip import Model\n",
    "\n",
    "# from pyscipopt.scip import Model as SCIPModel\n",
    "\n",
    "\n",
    "def maybe_raise_sigint(m: Model) -> None:\n",
    "    \"\"\"Manually check if SCIP encountered a sigint\"\"\"\n",
    "    if m.as_pyscipopt().getStatus() == \"userinterrupt\":\n",
    "        raise KeyboardInterrupt from None\n",
    "\n",
    "\n",
    "def rollout(\n",
    "    p: Model,\n",
    "    env: Branching,\n",
    "    branchrule: BranchRule,\n",
    "    *,\n",
    "    delay: bool = True,\n",
    "    kwargs: dict = None,\n",
    ") -> Iterable:\n",
    "    kwargs = {} if kwargs is None else kwargs\n",
    "    do_branch = branchrule(env)\n",
    "\n",
    "    obs, act_set, rew, fin, nfo = env.reset(p)\n",
    "    maybe_raise_sigint(env.model)\n",
    "    while not fin:\n",
    "        # the action set should be treated as a part of the observation\n",
    "        obs_ = Observation(obs, act_set)\n",
    "\n",
    "        # query the expert and branch\n",
    "        with DelaySIGINT(disable=not delay):\n",
    "            act_ = do_branch(obs_, **kwargs)\n",
    "            obs, act_set, rew, fin, nfo = env.step(act_)  # t -->> t+1\n",
    "            if not delay:\n",
    "                maybe_raise_sigint(env.model)\n",
    "\n",
    "            # send out the `x_{t-1}, a_{t-1}, r_t`\n",
    "            yield obs_, act_, rew  # XXX no underscore in `rew`!\n",
    "            # XXX SCIP has a complex node selection strategy, which even\n",
    "            #  when set to prioritize DFS, still may switch no arbitrary\n",
    "            #  node after branching. For the purpose of this experiment\n",
    "            #  we make the worst-case assumption about the transition\n",
    "            #  funciton that the next focus node is not at all related\n",
    "            #  to the a prior branching decision. The only assumption is\n",
    "            #  that the returned reward reflects the quality of the branching\n",
    "            #  decision\n",
    "\n",
    "    # no need to yield anything on fin=True, since ecole's terminal\n",
    "    #  observation is None\n",
    "    return nfo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be701ad8",
   "metadata": {},
   "source": [
    "co_it = ec.instance.CombinatorialAuctionGenerator()\n",
    "\n",
    "data = []\n",
    "for obs, act, rew in rollout(next(co_it), make_env(), strongbranch()):\n",
    "    data.append((obs, act, rew))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e505599",
   "metadata": {},
   "source": [
    "During evaluation we only care about the final `nfo` data from the branching env, as it contains the post-search tree stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    p: Model, env: Branching, branchrule: BranchRule, *, delay: bool = True\n",
    ") -> dict[str, float]:\n",
    "    try:\n",
    "        # use while-loop to capture the return value from the generator\n",
    "        #  (which we care about now).\n",
    "        it, n_steps = rollout(p, env, branchrule, delay=delay), 0\n",
    "        while True:\n",
    "            next(it)\n",
    "            n_steps += 1\n",
    "\n",
    "    except StopIteration as e:\n",
    "        return dict(n_requests=n_steps, **e.value)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75456d96",
   "metadata": {},
   "source": [
    "co_it = ec.instance.CombinatorialAuctionGenerator()\n",
    "\n",
    "evaluate(next(co_it), make_env(), strongbranch())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e636de",
   "metadata": {},
   "source": [
    "We use infinite problem generators, for which we implement a continuous rollout wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2af336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_rollout(\n",
    "    it: Iterable[Model], env: Branching, branchrule: BranchRuleCallable\n",
    ") -> Iterable:\n",
    "    # we use for-loop in case an infinite generator is actually finite\n",
    "    for p in it:\n",
    "        yield from rollout(p, env, branchrule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75044673",
   "metadata": {},
   "source": [
    "A problem instance server for mutlithreaded version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Job(NamedTuple):\n",
    "    p: Model\n",
    "    parameters: dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d1c03",
   "metadata": {},
   "source": [
    "A multithreaded version for especially slow branchrules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Full as QueueFull\n",
    "\n",
    "\n",
    "class RolloutPool:\n",
    "    def __init__(\n",
    "        self,\n",
    "        jobs: Queue,\n",
    "        factories: tuple[Callable, BranchRule],\n",
    "        *,\n",
    "        maxsize: int = 64,\n",
    "        timeout: float = 0.5,\n",
    "    ) -> None:\n",
    "        self.jobs, self.timeout = jobs, timeout\n",
    "\n",
    "        # spawn branching workers\n",
    "        self.workers = []\n",
    "        for factory, branchrule in factories:\n",
    "            t = Thread(target=self.worker, args=(factory, branchrule), daemon=True)\n",
    "            self.workers.append(t)\n",
    "\n",
    "        self.errors, self.output = Queue(), Queue(maxsize)\n",
    "        self.is_finished = Event()\n",
    "\n",
    "    def start(self) -> None:\n",
    "        for t in self.workers:\n",
    "            t.start()\n",
    "\n",
    "    def stop(self) -> None:\n",
    "        self.is_finished.set()\n",
    "        for t in self.workers:\n",
    "            t.join()\n",
    "\n",
    "        self.workers.clear()\n",
    "\n",
    "    def worker(self, factory: Callable, branchrule: BranchRule) -> None:\n",
    "        env = factory()\n",
    "        try:\n",
    "            while not self.is_finished.is_set():\n",
    "                # busy-check the termination flag until we receive a job\n",
    "                try:\n",
    "                    input = self.jobs.get(True, timeout=self.timeout)\n",
    "\n",
    "                except QueueEmpty:\n",
    "                    continue\n",
    "\n",
    "                assert isinstance(input, Job)\n",
    "\n",
    "                # do a rollout, sending the results into the buffer\n",
    "                for output in rollout(\n",
    "                    input.p,\n",
    "                    env,\n",
    "                    branchrule,  # XXX get a new do_branch on eeach new rollout\n",
    "                    delay=False,\n",
    "                    kwargs=input.parameters,\n",
    "                ):\n",
    "                    while not self.is_finished.is_set():\n",
    "                        try:\n",
    "                            self.output.put(output, True, timeout=self.timeout)\n",
    "\n",
    "                        except QueueFull:\n",
    "                            continue\n",
    "\n",
    "                        break\n",
    "\n",
    "        except BaseException as e:\n",
    "            self.is_finished.set()\n",
    "            self.errors.put(e)\n",
    "\n",
    "    def __iter__(self) -> None:\n",
    "        # keep regularly checking the termination flag until\n",
    "        #  we receive a result\n",
    "        while not self.is_finished.is_set():\n",
    "            try:\n",
    "                yield self.output.get(True, timeout=self.timeout)\n",
    "\n",
    "            except QueueEmpty:\n",
    "                continue\n",
    "\n",
    "        self.stop()\n",
    "        if not self.errors.empty():\n",
    "            raise self.errors.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70534898",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141b17b",
   "metadata": {},
   "source": [
    "## the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac1a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782cb876",
   "metadata": {},
   "source": [
    "A tracer hook to help with debugging live models\n",
    "\n",
    "```python\n",
    "hook = model.register_forward_pre_hook(tracer)\n",
    "\n",
    "bt, by = next(feed)\n",
    "model(bt, by)\n",
    "\n",
    "...\n",
    "\n",
    "hook.remove()\n",
    "```\n",
    "- _judicious placement_ of breakpoints `b` is advised so that continuation `c` would work\n",
    "    - try `n`, `b 1208`\n",
    "- otherwise use `n` for next, `s` for step inside, `u/d` to move between stack frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracer(module, input) -> None:\n",
    "    import pdb\n",
    "\n",
    "    pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def71b8",
   "metadata": {},
   "source": [
    "A good old trusted MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(activation: type = nn.ReLU, /, *n_dims: int) -> nn.Sequential:\n",
    "    \"\"\"multi-layer perceptrons are magic\"\"\"\n",
    "    layers = []\n",
    "    for d0, d1 in zip(n_dims, n_dims[1:]):\n",
    "        layers.append(nn.Linear(d0, d1))\n",
    "        layers.append(activation())\n",
    "\n",
    "    return nn.Sequential(*layers[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1663a59",
   "metadata": {},
   "source": [
    "The original message passing architecture is\n",
    "$$\n",
    "x^k_i\n",
    "    = \\gamma\\bigl(\n",
    "        x^{k-1}_i,\n",
    "        \\operatorname{\\diamond} \\bigl(\n",
    "            \\bigl\\{\n",
    "                \\phi(x^{k-1}_i, x^{k-1}_j, e_{ij})\n",
    "                \\colon j \\in G_i\n",
    "            \\bigr\\}\n",
    "        \\bigr)\n",
    "    \\bigr)\n",
    "    \\,, $$\n",
    "\n",
    "where $\\diamond$ is a permutation-invariant set-to-real _aggregation_ operator,\n",
    "$\\phi$ is the _message_ function, and $\\gamma$ is the _output_ function, and $G_u$\n",
    "is the graph neighborhood of the vertex $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97994190",
   "metadata": {},
   "source": [
    "Below we implement a variant of the massage passing where the _aggregation_ and _message_ operations are based on multi-headed cross attention. This means, that $\\diamond$ and $\\phi$ are _fused_:\n",
    "$$\n",
    "    (\\diamond \\circ \\phi)\n",
    "    \\colon \\mathbb{R}^d \\times \\bigl(\\mathbb{R}^d\\bigr)^* \\to \\mathbb{R}^d\n",
    "    \\colon (x_i, \\{x_j\\colon j \\in G_i\\}) \\mapsto\n",
    "        \\sum_{j \\in G_i} p_j \\phi^v(x_j)\n",
    "    \\,. $$\n",
    "with $\\log p_j \\propto \\phi^q(x_i)^\\top \\phi^k(x_j)$ for $j \\in G_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f31923",
   "metadata": {},
   "source": [
    "Implement a special layer that computes message passing with cross-attention between the parts of a bipartite graph. We make heavy use of `torch-scatter` operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1347483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "from einops import rearrange\n",
    "from torch_scatter import scatter_softmax, scatter_sum\n",
    "\n",
    "\n",
    "class BipartiteMHXA(Module):\n",
    "    def __init__(self, n_embed: int = 32, n_heads: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # query projection for `input`\n",
    "        self.p_q = nn.Linear(n_embed, n_embed, bias=False)\n",
    "\n",
    "        # key-value projection for `other`\n",
    "        self.p_kv = nn.Linear(n_embed, 2 * n_embed, bias=False)\n",
    "\n",
    "        # the final head output mixer\n",
    "        self.out = nn.Linear(n_embed, n_embed)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input: Tensor,\n",
    "        other: Tensor,\n",
    "        coupling: tuple[Tensor, Tensor],\n",
    "        weights: Tensor = None,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Cross attention form `input` (query) to `other` (keys and values)\"\"\"\n",
    "        if weights is not None:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # `coupling` specifies which `input` attends to which `other`\n",
    "        #  XXX (u, v) represents a directed edge `u to v` and means\n",
    "        #  that `input[u]` attends to `other[v]` and sources data from it.\n",
    "        t, s = coupling  # XXX t (in input) <<-- s (in other)\n",
    "\n",
    "        # get the qkv vectors, properly reshaped for multi-headed attention\n",
    "        q = rearrange(self.p_q(input), \"N (h f) -> N h () f\", h=self.n_heads)\n",
    "        k, v = self.p_kv(other).chunk(2, -1)\n",
    "        k = rearrange(k, \"N (h f) -> N h f ()\", h=self.n_heads)\n",
    "        v = rearrange(v, \"N (h f) -> N h f\", h=self.n_heads)\n",
    "\n",
    "        # q is N x H x 1 x F, k is N x H x F x 1\n",
    "        # XXX indexing by `t` and `s` materializes potentially large tensors!\n",
    "        score = torch.matmul(q[t], k[s]).div(sqrt(q.shape[-1])).squeeze(-1)\n",
    "\n",
    "        # softmax over `a` in `(q_b k_{i_a})_{a \\colon j_a = b}`\n",
    "        # XXX `scatter_softmax(x, j)` computes softmax over `x` grouped by j\n",
    "        #    `y[j==b] = softmax(x[j==b])` for all b in j's range\n",
    "        # XXX `q[b].matmul(k[i[j==b]])`\n",
    "        alpha = scatter_softmax(score, t, dim=0, dim_size=len(q))\n",
    "\n",
    "        # `scatter_sum(x, j)` computes `y_b = \\sum_{a: j_a = b} x_a`\n",
    "        # XXX `y[j[a]] += x[a]`\n",
    "        out = scatter_sum(alpha * v[s], t, dim=0, dim_size=len(q))\n",
    "        return self.out(rearrange(out, \"... h f -> ... (h f)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70273f92",
   "metadata": {},
   "source": [
    "A bipartite transformer block\n",
    "* we opt for post-norm architecture\n",
    "* [Zhang et al. (2022)](https://arxiv.org/abs/2206.11925.pdf) metnion that norm-first layer norm may impact expressivty, but do not analyze when the norm is at the other end\n",
    "* while [](https://arxiv.org/abs/2002.04745.pdf) claim that pre-LN stabilizes the grad, whereas post-LN expected grads' magnitude grows with the depth\n",
    "  - strange, but this appears to contradict their own theoreical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe48bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BipartiteBlock(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int = 32,\n",
    "        n_heads: int = 1,\n",
    "        p_drop: float = 0.2,\n",
    "        b_norm_first: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.b_norm_first = b_norm_first\n",
    "\n",
    "        self.attn = BipartiteMHXA(n_embed, n_heads)\n",
    "        self.do_1 = nn.Dropout(p_drop)\n",
    "        self.ln_1 = nn.LayerNorm(n_embed)\n",
    "\n",
    "        self.pwff = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "        )\n",
    "        self.do_2 = nn.Dropout(p_drop)\n",
    "        self.ln_2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input: Tensor,\n",
    "        other: Tensor,\n",
    "        coupling: tuple[Tensor, Tensor],\n",
    "        weights: Tensor = None,\n",
    "    ) -> Tensor:\n",
    "        x = input\n",
    "        if self.b_norm_first:\n",
    "            x = x + self.do_1(self.attn(self.ln_1(x), other, coupling, weights))\n",
    "            return x + self.do_2(self.pwff(self.ln_2(x)))\n",
    "\n",
    "        x = self.ln_1(x + self.do_1(self.attn(x, other, coupling, weights)))\n",
    "        return self.ln_2(x + self.do_2(self.pwff(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce1711",
   "metadata": {},
   "source": [
    "The full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_log_softmax, scatter_logsumexp\n",
    "from torch_scatter import scatter_max, scatter_mean\n",
    "\n",
    "\n",
    "class NeuralVariableSelector(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dim_vars: int = 19,\n",
    "        n_dim_cons: int = 5,\n",
    "        n_embed: int = 32,\n",
    "        n_heads: int = 1,\n",
    "        n_blocks: int = 1,\n",
    "        p_drop: float = 0.2,\n",
    "        b_norm_first: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleDict(\n",
    "            dict(\n",
    "                vars=mlp(nn.LeakyReLU, n_dim_vars, 4 * n_embed, n_embed),\n",
    "                cons=mlp(nn.LeakyReLU, n_dim_cons, 4 * n_embed, n_embed),\n",
    "                # edge=mlp(nn.LeakyReLU, 1, 4 * n_embed, n_embed)\n",
    "                edge=None,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        blk = [\n",
    "            BipartiteBlock(n_embed, n_heads, p_drop, b_norm_first)\n",
    "            for _ in range(n_blocks)\n",
    "        ]\n",
    "        self.block_cv = nn.ModuleList(blk)\n",
    "        blk = [\n",
    "            BipartiteBlock(n_embed, n_heads, p_drop, b_norm_first)\n",
    "            for _ in range(n_blocks)\n",
    "        ]\n",
    "        self.block_vc = nn.ModuleList(blk)\n",
    "\n",
    "        self.head = mlp(nn.LeakyReLU, 2 * n_embed, 4 * n_embed, 1)\n",
    "\n",
    "    def forward(\n",
    "        self, input: BatchObservation, target: Tensor = None\n",
    "    ) -> tuple[Tensor, dict[str, Tensor]]:\n",
    "        jc, jv = input.ctov_ij\n",
    "\n",
    "        # encode the vars and cons features\n",
    "        cons = self.encoder.cons(input.cons)\n",
    "        vars = self.encoder.vars(input.vars)\n",
    "        edge = None\n",
    "        if self.encoder.edge is not None:\n",
    "            edge = self.encoder.edge(input.ctov_v)\n",
    "\n",
    "        # bipartite ladder\n",
    "        for m_cv, m_vc in zip(self.block_cv, self.block_vc):\n",
    "            cons = m_cv(cons, vars, (jc, jv), edge)\n",
    "            vars = m_vc(vars, cons, (jv, jc), edge)\n",
    "\n",
    "        # compute the graph-level variable embedding\n",
    "        graph = scatter_mean(vars, input.inx_vars, 0)\n",
    "\n",
    "        # get the raw-logit scores of each variable\n",
    "        x = torch.cat((vars, graph[input.inx_vars]), -1)\n",
    "        raw = self.head(x).squeeze(-1)\n",
    "\n",
    "        # optionally compute the loss\n",
    "        if target is None:\n",
    "            return raw, {}  # raw.new_full((), float(\"nan\"))\n",
    "\n",
    "        # get the log-probas and compute the loss terms\n",
    "        logp_vars = scatter_log_softmax(raw, input.inx_vars)\n",
    "\n",
    "        # get the log-likelihood of the target variables\n",
    "        # XXX `ptr_vars` corrects the variable indices in the target batch\n",
    "        j = input.ptr_vars[:-1] + target.to(logp_vars.device)\n",
    "        loglik_target = logp_vars[j]\n",
    "\n",
    "        # get the log likelihood of the forbidden variable set\n",
    "        mask = torch.ones_like(logp_vars, dtype=bool)\n",
    "        mask[input.actset] = False\n",
    "        logp_forbidden = scatter_logsumexp(\n",
    "            logp_vars[mask],\n",
    "            input.inx_vars[mask],  # XXX group by batch assignment\n",
    "            dim=0,\n",
    "            # It is possible that the forbidden mask to be\n",
    "            #  empty in certain nodes of certain instances\n",
    "            # XXX ptr_* is one longer than the batch size\n",
    "            dim_size=len(input.ptr_vars) - 1,\n",
    "        )\n",
    "\n",
    "        # XXX no need to compute either the act-set sizes\n",
    "        #  or the vars sizes with `input.ptr_vars.diff()`\n",
    "        # n_forbidden_size = scatter_sum(mask.float(), input.inx_vars, 0)\n",
    "        loglik_actset = logp_forbidden  # .div(n_forbidden_size)\n",
    "\n",
    "        # compute the discrete entropy $- \\sum_j \\pi_j \\log \\pi_j$\n",
    "        # XXX For input x and target y, `F.kl_div(x, y, \"none\", log_target=True)`\n",
    "        #  returns $e^y (y - x)$, correctly handling infinite `-ve` logits.\n",
    "        #  `log_target` relates to `y` being logits (True) or probas (False).\n",
    "        # XXX `.new_zeros(())` creates a scalar zero (yes, an EMPTY tuple)\n",
    "        # XXX from nle-toolbox\n",
    "        zero = logp_vars.new_zeros(())\n",
    "        p_log_p = F.kl_div(zero, logp_vars, reduction=\"none\", log_target=True)\n",
    "        entropy = scatter_sum(p_log_p, input.inx_vars, 0).neg()\n",
    "\n",
    "        return raw, dict(\n",
    "            neg_target=loglik_target.neg(),\n",
    "            neg_actset=loglik_actset.neg(),\n",
    "            entropy=entropy,\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict(self, input: BatchObservation) -> ndarray:\n",
    "        scores, _ = self.forward(input)\n",
    "\n",
    "        # mask forbidden variables\n",
    "        mask = torch.ones_like(scores, dtype=bool)\n",
    "        mask[input.actset] = False\n",
    "        scores.masked_fill_(mask, float(\"-inf\"))\n",
    "\n",
    "        # pick the maximally probable action in each batch item\n",
    "        _, j = scatter_max(scores, input.inx_vars, 0)\n",
    "        if mask[j].any():\n",
    "            raise RuntimeError(\"Empty actset!\")  # sanity check\n",
    "\n",
    "        # subtract the base index of each batch item\n",
    "        j -= input.ptr_vars[:-1]\n",
    "        return j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b75669",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce3f4da",
   "metadata": {},
   "source": [
    "Some procs for handling dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99866c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_dict(dict: dict[..., dict]) -> dict[..., dict]:\n",
    "    outer = {}\n",
    "    for k_out, inner in dict.items():\n",
    "        for k_in, value in inner.items():\n",
    "            new = outer.setdefault(k_in, {})\n",
    "            assert k_out not in new\n",
    "            new[k_out] = value\n",
    "\n",
    "    return outer\n",
    "\n",
    "\n",
    "def collate_dict(records: list[dict]) -> dict[..., list]:\n",
    "    \"\"\"Collate records assuming no fields are missing\"\"\"\n",
    "    out = {}\n",
    "    for record in records:\n",
    "        for field, value in record.items():\n",
    "            out.setdefault(field, []).append(value)\n",
    "\n",
    "    return {k: np.array(v) for k, v in out.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee12112",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd57a29",
   "metadata": {},
   "source": [
    "## Trainnig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c5c51",
   "metadata": {},
   "source": [
    "Allow for 100k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d323ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total, n_batch_size = 100_000, 16  # XXX 100k is too long\n",
    "n_reservoir = 512  # XXX 128 reservoir was ok for CAuc\n",
    "\n",
    "C_neg_actset = 0.0  # 1e-3  # XXX the orginal CAuc used to have 0.0\n",
    "C_entropy = 0.0  # 1e-2  # XXX was set to zero in the first CAuc\n",
    "\n",
    "# use a seed sequence with a fixed entropy pool\n",
    "ss = SeedSequence(83278314352113072500167414370310027453)\n",
    "# ss = SeedSequence(None)  # use `ss.entropy` for future reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe36b8f",
   "metadata": {},
   "source": [
    "Pipe the generator that mixes several the CO problems into the continupus rollout iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.benchmarks import gasse2019\n",
    "from ecole.instance import CombinatorialAuctionGenerator\n",
    "\n",
    "\n",
    "# init the branching env\n",
    "env = make_env(spawn_one(ss))\n",
    "\n",
    "# CAuc(100, 500), CAuc(50, 250)\n",
    "gens = [\n",
    "    CombinatorialAuctionGenerator(100, 500, rng=ecole_seed(spawn_one(ss))),\n",
    "    CombinatorialAuctionGenerator(50, 250, rng=ecole_seed(spawn_one(ss))),\n",
    "]\n",
    "\n",
    "# Use co problems from Gasse 2019\n",
    "# gens = transpose_dict(gasse2019(spawn_one(ss)))\n",
    "# gens = gens[\"train\"].values()\n",
    "itco = mixer(*gens, seed=spawn_one(ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c4d22",
   "metadata": {},
   "source": [
    "set up the rollout observation feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "if False:\n",
    "    feed = continuous_rollout(itco, env, strongbranch())\n",
    "\n",
    "else:\n",
    "\n",
    "    def t_job_generator(it: Iterable, jobs: Queue) -> None:\n",
    "        for sample in it:\n",
    "            jobs.put(Job(sample, {}))\n",
    "\n",
    "    n_jobs = 6\n",
    "    jobs = Queue(max(16, 2 * n_jobs))\n",
    "    source = Thread(\n",
    "        target=t_job_generator,\n",
    "        args=(itco, jobs),\n",
    "        daemon=True,\n",
    "    )\n",
    "    source.start()\n",
    "\n",
    "    branchrule = strongbranch()\n",
    "    pool = RolloutPool(\n",
    "        jobs,\n",
    "        [(partial(make_env, fork), branchrule) for fork in ss.spawn(n_jobs)],\n",
    "        maxsize=64,\n",
    "        timeout=0.5,\n",
    "    )\n",
    "    pool.start()\n",
    "\n",
    "    feed = iter(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0734367",
   "metadata": {},
   "source": [
    "Then feed the branching observation data into a shuffler, limiter and then batcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f6118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "# set up the data stream\n",
    "# XXX if we put a limiter on the source iter, then shuffle's reservoir\n",
    "#  would consume `n_reservoir`, which are never going to be yield,\n",
    "#  if the source is infinite. In this case, instead, we should put\n",
    "#  a limiter on the `shuffle` itself.\n",
    "it = feed\n",
    "it = limit(it, trange(n_total, ncols=70))\n",
    "it = shuffle(it, n_reservoir, spawn_one(ss))\n",
    "it = batch(it, n_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858e728",
   "metadata": {},
   "source": [
    "Set up the model and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = NeuralVariableSelector(19, 5, 32, 2, 1, 0.2)\n",
    "# hook = mod.register_forward_pre_hook(tracer)\n",
    "\n",
    "optim = torch.optim.AdamW(mod.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59b79d97",
   "metadata": {},
   "source": [
    "# try the branching server\n",
    "server = BranchingServer(batched_ml_branchrule(mod))\n",
    "server.start()\n",
    "\n",
    "cotest = CombinatorialAuctionGenerator(n_items=50, n_bids=250)\n",
    "env = make_env()\n",
    "nfo = evaluate(next(cotest), env, server.connect, delay=False)\n",
    "\n",
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c425f",
   "metadata": {},
   "source": [
    "train an IL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f1171",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "log = dict()\n",
    "for bt in it:\n",
    "    # collate the batch\n",
    "    obs, act, rew = zip(*bt)\n",
    "    bx = collate(obs, device=device)\n",
    "    act = as_tensor(np.array(act, dtype=int), device=device)\n",
    "    rew = np.array(rew, dtype=np.float32)\n",
    "\n",
    "    # forward pass\n",
    "    mod.train()\n",
    "    _, terms = mod(bx, target=act)\n",
    "    terms = {k: v.mean() for k, v in terms.items()}\n",
    "\n",
    "    # log likelihoods and entropy have the same uints (and thus scale)\n",
    "    loss = (\n",
    "        # (min) -ve log-likelihood\n",
    "        terms[\"neg_target\"]\n",
    "        # (max) entropy\n",
    "        - C_entropy * terms[\"entropy\"]\n",
    "        # (max) -ve log-likelihood of forbidden actions\n",
    "        - C_neg_actset * terms[\"neg_actset\"]\n",
    "    )\n",
    "\n",
    "    # backprop\n",
    "    mod.zero_grad(True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    mod.eval()\n",
    "    do_add({k: float(v) for k, v in terms.items()}, log)\n",
    "\n",
    "    clear_output(True)\n",
    "    fig, ax0 = plt.subplots(1, 1, figsize=(5, 2), dpi=200, sharex=True)\n",
    "    plot_stats(ax0, **log)\n",
    "    ax0.set_title(\"terms\")\n",
    "    ax0.legend(loc=\"lower left\", fontsize=\"xx-small\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41875b1c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime\n",
    "\n",
    "torch.save(\n",
    "    dict(\n",
    "        __dttm__=strftime(\"%Y%m%d-%H%M%S\"),\n",
    "        state_dict=mod.state_dict(),\n",
    "    ),\n",
    "    #     \"CAuc100500-50250-good.pt\"\n",
    "    #     \"second-good.pt\",\n",
    "    #     \"second-good--zero-actset--logp.pt\",\n",
    "    #     \"second-CAuc100500-50250-good-no-reg.pt\",\n",
    "    #     \"second-CAuc50250-good-no-reg.pt\",\n",
    "    #     \"second-CAuc50250-good-no-reg-sum.pt\",\n",
    "    #     \"third-CAuc-dropout.pt\",\n",
    "    #     \"third-CAuc-dropout__large.pt\",\n",
    "    \"fouth-All-dropout.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159bd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = NeuralVariableSelector(19, 5, 32, 2, 1, 0.2)\n",
    "\n",
    "ckpt = torch.load(\"fouth-All-dropout.pt\")\n",
    "mod.load_state_dict(ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332838a8",
   "metadata": {},
   "source": [
    "List all the rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b89b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = {\n",
    "    \"trained\": ml_branchrule(mod),\n",
    "    #     \"strongbranch\": strongbranch(),\n",
    "    #     \"pseudocostbranch\": strongbranch(True),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b535b73c",
   "metadata": {},
   "source": [
    "Evaluate the branchrules in parallel threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39052fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_evaluate(rk: int, ws: int, ss: SeedSequence, rx: Queue, tx: Queue) -> None:\n",
    "    env = make_env(ss, presolve=True)\n",
    "    while True:\n",
    "        p = rx.get()\n",
    "        if not isinstance(p, Model):\n",
    "            break\n",
    "\n",
    "        out = {}\n",
    "        for nom, rule in rules.items():\n",
    "            out[nom] = evaluate(p.copy_orig(), env, rule, delay=False)\n",
    "\n",
    "        tx.put(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f9116",
   "metadata": {},
   "source": [
    "Let's do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735d9d8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "n_jobs = 8\n",
    "\n",
    "# spawn thread workers consuming from `rx` and producing into `tx`\n",
    "threads, rx, tx = [], Queue(2 * n_jobs), Queue()\n",
    "for rk, sk in enumerate(ss.spawn(n_jobs)):\n",
    "    args = rk, n_jobs, sk, rx, tx\n",
    "    t = Thread(target=t_evaluate, args=args, daemon=True)\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "# start filling the rx queue\n",
    "print(\"Populating the jobs queue\")\n",
    "it_co = CombinatorialAuctionGenerator(100, 500)\n",
    "for p, _ in zip(it_co, trange(1000, ncols=70)):\n",
    "    rx.put(p)\n",
    "\n",
    "# submit termination signals through the queue and wait for shutdown\n",
    "print(\"Shutting workers down\")\n",
    "for t in threads:\n",
    "    rx.put(StopIteration)\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "# collect the evaluation results\n",
    "nfos = {}\n",
    "while not tx.empty():\n",
    "    do_add(tx.get(), nfos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8392572",
   "metadata": {},
   "source": [
    "* `n_nodes`, `n_requests`\n",
    "* `n_lpiter`\n",
    "* `f_soltime`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0967a23f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime\n",
    "\n",
    "# raise RuntimeError\n",
    "\n",
    "__dttm__ = strftime(\"%Y%m%d-%H%M%S\")\n",
    "metrics = {k: collate_dict(nfo) for k, nfo in nfos.items()}\n",
    "torch.save(\n",
    "    dict(__dttm__=__dttm__, metrics=metrics),\n",
    "    #     \"cauc__il_xattn__zero_logactst_reg.pt\",\n",
    "    #     \"cauc__il_xattn__nonzero_logactst_reg.pt\",\n",
    "    #     \"cauc__il_xattn__no-reg.pt\",\n",
    "    #     \"cauc50250__il_xattn__no-reg.pt\",\n",
    "    #     \"cauc50250__il_xattn__no-reg_sum.pt\",\n",
    "    #     \"cauc__il_xattn__dropout.pt\",\n",
    "    #     \"cauc__il_xattn__dropout__large.pt\",\n",
    "    \"all__il_xattn__dropout.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec17c7",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4557850",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_zer = torch.load(\"cauc__il_xattn__zero_logactst_reg.pt\")[\"metrics\"]\n",
    "m_nnz = torch.load(\"cauc__il_xattn__nonzero_logactst_reg.pt\")[\"metrics\"]\n",
    "m_org = torch.load(\"cauc__il_xattn__no-reg.pt\")[\"metrics\"]\n",
    "m_smol = torch.load(\"cauc50250__il_xattn__no-reg.pt\")[\"metrics\"]\n",
    "m_smol_sum = torch.load(\"cauc50250__il_xattn__no-reg_sum.pt\")[\"metrics\"]\n",
    "m_dropout = torch.load(\"cauc__il_xattn__dropout.pt\")[\"metrics\"]\n",
    "m_large = torch.load(\"cauc__il_xattn__dropout__large.pt\")[\"metrics\"]\n",
    "m_all = torch.load(\"all__il_xattn__dropout.pt\")[\"metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d09386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# series = \"n_nodes\", \"n_requests\", \"f_soltime\"\n",
    "series = \"n_nodes\"\n",
    "\n",
    "metric = {\n",
    "    \"nonzero\": m_nnz[\"trained\"][series],\n",
    "    \"zero\": m_zer[\"trained\"][series],\n",
    "    \"original\": m_org[\"trained\"][series],\n",
    "    \"small\": m_smol[\"trained\"][series],\n",
    "    \"small-sum\": m_smol_sum[\"trained\"][series],\n",
    "    \"dropout\": m_dropout[\"trained\"][series],\n",
    "    \"large\": m_large[\"trained\"][series],\n",
    "    \"all\": m_all[\"trained\"][series],\n",
    "    \"strongbranch\": np.r_[\n",
    "        m_nnz[\"strongbranch\"][series],\n",
    "        m_zer[\"strongbranch\"][series],\n",
    "        m_org[\"strongbranch\"][series],\n",
    "        m_smol[\"strongbranch\"][series],\n",
    "        m_smol_sum[\"strongbranch\"][series],\n",
    "    ],\n",
    "    \"pseudocostbranch\": np.r_[\n",
    "        m_nnz[\"pseudocostbranch\"][series],\n",
    "        m_zer[\"pseudocostbranch\"][series],\n",
    "        m_org[\"pseudocostbranch\"][series],\n",
    "        m_smol[\"pseudocostbranch\"][series],\n",
    "        m_smol_sum[\"pseudocostbranch\"][series],\n",
    "    ],\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    \"strongbranch\": \"black\",\n",
    "    \"pseudocostbranch\": \"C1\",\n",
    "    \"nonzero\": \"C0\",\n",
    "    \"zero\": \"C2\",\n",
    "    \"original\": \"C3\",\n",
    "    \"small\": \"C4\",\n",
    "    \"small-sum\": \"C5\",\n",
    "    \"dropout\": \"C6\",\n",
    "    \"large\": \"C7\",\n",
    "    \"all\": \"C8\",\n",
    "}\n",
    "xlat = {\n",
    "    \"strongbranch\": \"SB\",\n",
    "    \"pseudocostbranch\": \"PC\",\n",
    "    \"zero\": \"IL-XAttn-ent\",\n",
    "    \"original\": \"IL-XAttn-original\",\n",
    "    \"nonzero\": \"IL-XAttn-act-set-ent\",\n",
    "    \"small\": \"IL-XAttn-50250\",\n",
    "    \"small-sum\": \"IL-XAttn-50250-sum\",\n",
    "    \"dropout\": \"IL-XAttn-dropout\",\n",
    "    \"large\": \"IL-XAttn-dropout-64\",\n",
    "    \"all\": \"all-IL-XAttn-dropout\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404585a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"strongbranch\"\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5), dpi=200)\n",
    "for name, data in metric.items():\n",
    "    if base == name:\n",
    "        continue\n",
    "    p, q = pp_curve(x=metric[base], y=data, num=None)\n",
    "    ax.plot(p, q, label=xlat[name], c=colors[name])\n",
    "\n",
    "ax.plot((0, 1), (0, 1), c=colors[base], zorder=10, alpha=0.25, label=xlat[base])\n",
    "ax.set_xlim(-0.025, 1.025)\n",
    "ax.set_ylim(-0.025, 1.025)\n",
    "ax.set_aspect(1.0)\n",
    "ax.legend(loc=\"best\", fontsize=\"xx-small\")\n",
    "\n",
    "fig.savefig(f\"cauc__{__dttm__}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6db3a12",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0541e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mod.eval()\n",
    "scores = mod(bx)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 3), dpi=300)\n",
    "\n",
    "probas = scatter_softmax(scores, x.inx_vars, 0)\n",
    "ax.plot(probas.detach().numpy())\n",
    "ax.set_xlim(-1.5, len(scores) + 1.5)\n",
    "\n",
    "n_spans = int(x.inx_vars.max()) + 1\n",
    "f, colors = 0, (\"red\", \"blue\")\n",
    "for s in range(n_spans):\n",
    "    jx = (x.inx_vars == s).nonzero()\n",
    "    a, b = int(jx.min()), int(jx.max())\n",
    "\n",
    "    ax.axvspan(a, b, color=colors[f], alpha=0.05)\n",
    "    f = 1 - f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08278268",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
