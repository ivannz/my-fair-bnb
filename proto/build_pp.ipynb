{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4a90ea",
   "metadata": {},
   "source": [
    "# PP-plot builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7505b82",
   "metadata": {},
   "source": [
    "The pp-curve drawing procedure. We compare distibutions using the pp-curve, which is analogous to the ROC curve: pp compares two independent distributions, while ROC compares the true-, false- positive distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toybnb.scip.ecole.il.plotting import pp_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57f876",
   "metadata": {},
   "source": [
    "Shifted geometric mean for $\\varepsilon > 0$ (used in Gasse et al. 2019):\n",
    "    $$\n",
    "    \\operatorname{sgm}(x)\n",
    "        = \\exp\\bigl\\{\n",
    "            \\frac1m \\sum_j \\log \\max\\{1, x_j + \\varepsilon\\}\n",
    "        \\bigr\\} - \\varepsilon\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62697622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geommean(arr: ndarray, axis: int = None, *, eps: float = 1.0) -> ndarray:\n",
    "    r\"\"\"Shifted geometric mean of `arr` for `$\\varepsilon > 0$ (`eps) as\n",
    "    used in Gasse et al. 2019:\n",
    "    $$\n",
    "    \\operatorname{sgm}(x)\n",
    "        = \\exp\\bigl\\{\n",
    "            \\frac1m \\sum_j \\log \\max\\{1, x_j + \\varepsilon\\}\n",
    "        \\bigr\\} - \\varepsilon\n",
    "    \\,. $$\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.log(np.maximum(arr + eps, 1.0))\n",
    "    return np.exp(np.mean(x, axis=axis)) - eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcece9e",
   "metadata": {},
   "source": [
    "We keep experiment data in CSV form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def read_csv(filename: str) -> Iterable[dict]:\n",
    "    \"\"\"Read the csv as an iterable of dicts\"\"\"\n",
    "    # use stdlib's csv package to read the comma-separated molecular data\n",
    "    with open(filename, \"rt\", newline=\"\") as f:\n",
    "        it = csv.reader(f, delimiter=\",\")\n",
    "\n",
    "        # get the header then represent rows a dicts\n",
    "        header = next(it)\n",
    "        for row in it:\n",
    "            yield dict(zip(header, row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6952d27",
   "metadata": {},
   "source": [
    "A simple proc to collate a list of identically structured dicts into a dict of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(records: list[dict]) -> dict[..., ndarray]:\n",
    "    \"\"\"Collate records assuming no fields are missing\"\"\"\n",
    "    out = {}\n",
    "    for rec in records:\n",
    "        for field, value in rec.items():\n",
    "            out.setdefault(field, []).append(value)\n",
    "\n",
    "    return {k: np.array(v) for k, v in out.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a85d6a5",
   "metadata": {},
   "source": [
    "The pp-plots are a comprehensive method to decide, which methods produce a superior number-of-nodes distribution. But in order to track the progress during training we use point-statistics, which are less comprehensive and do not show the bigger picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408632c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(metrics: dict, series: str) -> dict[str, dict[str, float]]:\n",
    "    tmp, out = {}, {}\n",
    "    for (k1, k2), v in metrics.items():\n",
    "        tmp.setdefault(k1, []).append(\n",
    "            dict(\n",
    "                tot=len(v[series]),\n",
    "                median=np.median(v[series]),\n",
    "                mean=np.mean(v[series]),\n",
    "                std=np.std(v[series]),\n",
    "                sgm=geommean(v[series], eps=1.0),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # collate and average across replications\n",
    "    for k, v in tmp.items():\n",
    "        for met, val in collate(v).items():\n",
    "            dt = out.setdefault(k, {})\n",
    "            dt[met] = np.mean(val)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9e0582",
   "metadata": {},
   "source": [
    "Load the stats from the csv in the format of Scavuzzo et al. 2022:\n",
    "- `policy` -- the identifier of the branching policy\n",
    "- `seed` -- the opaque id of the replication (fixed randomness of SCIP)\n",
    "- `type` -- the kind of evaluation: `test` and `transfer` -- in theiur original code, `custom` -- in our patches\n",
    "- `instance` -- the path to the instance used for evaluation\n",
    "- `nnodes` -- the total number of nodes after solving\n",
    "- `nlps` -- the number of LP solver iterarions\n",
    "- `stime` -- the solution time as measured by SCIP using CPU seconds (`clocktype=1`)\n",
    "- `gap` -- the primal-dual gap achieved at the end of the bnb search\n",
    "- `status` -- SCIPs reported solution status\n",
    "- `walltime` -- the wall time of the solution process as measured by python\n",
    "- `proctime` -- the cpu time of the solution process as measured by python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad89af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics_scavuzzo(\n",
    "    filename: str, *, keep: str = (\"test\", \"custom\")\n",
    ") -> dict[tuple, ndarray]:\n",
    "    \"\"\"Load the metrics data in Scavuzzo et al. 2022 format\"\"\"\n",
    "    keep = keep if isinstance(keep, tuple) else (keep,)\n",
    "\n",
    "    metrics = {}\n",
    "    # make sure NOT to pool values from different seeds,\n",
    "    #  as they are independent runs on the same instance\n",
    "    for rec in read_csv(filename):\n",
    "        if rec[\"type\"] not in keep:\n",
    "            continue\n",
    "\n",
    "        key = rec[\"policy\"], int(rec[\"seed\"])\n",
    "        metrics.setdefault(key, []).append(\n",
    "            {\n",
    "                \"n_nodes\": int(rec[\"nnodes\"]),\n",
    "                \"n_lps\": int(rec[\"nlps\"]),\n",
    "                \"f_soltime\": float(rec[\"stime\"]),\n",
    "                \"f_gap\": float(rec[\"gap\"]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {k: collate(v) for k, v in metrics.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b1264",
   "metadata": {},
   "source": [
    "Load the evaluation metrics data folder in rlbnb format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d773a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_rlbnb(rec: dict) -> dict:\n",
    "    \"\"\"Standardize the record given in rlbnb format\"\"\"\n",
    "    return dict(\n",
    "        # ignore the '' index\n",
    "        # lp_iterations=int(lp_iterations),\n",
    "        n_nodes=int(float(rec[\"num_nodes\"])),\n",
    "        f_soltime=float(rec[\"solving_time\"]),\n",
    "    )\n",
    "\n",
    "\n",
    "def rename_rlbnb(filename: str) -> str:\n",
    "    if filename.startswith((\"bipartite\", \"tripartite\", \"masked\")) and \"_\" in filename:\n",
    "        name, _, co = filename.partition(\"_\")\n",
    "        return name\n",
    "\n",
    "    if filename == \"strong\":\n",
    "        return \"internal:vanillafullstrong\"\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "def load_metrics_rlbnb(path: str) -> dict[tuple, ndarray]:\n",
    "    \"\"\"Load the data folder in rlbnb format\"\"\"\n",
    "    if os.path.isdir(path):\n",
    "        root, _, filenames = next(os.walk(os.path.abspath(path)))\n",
    "\n",
    "    elif os.path.isfile(path):\n",
    "        root, filename = os.path.split(os.path.abspath(path))\n",
    "        filenames = [filename]\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    metrics = {}\n",
    "    for basename, ext in map(os.path.splitext, filenames):\n",
    "        if ext != \".csv\":\n",
    "            continue\n",
    "\n",
    "        # load the csv and store is as single-seed evaluation result\n",
    "        # XXX make sure to call `rename_*` and `cast_*`\n",
    "        records = read_csv(os.path.join(root, basename + ext))\n",
    "        metrics[rename_rlbnb(basename), 0] = collate(map(cast_rlbnb, records))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcdfef8",
   "metadata": {},
   "source": [
    "Try to assign a unique fixed color to the each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17976eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scavuzzo_colors = {\n",
    "    \"internal:relpscost\": \"fuchsia\",\n",
    "    \"internal:vanillafullstrong\": \"k\",\n",
    "    # \"internal:emulated-vanillafullstrong\": \"C1\",\n",
    "    \"gcnn:il\": \"C2\",\n",
    "    \"gcnn:mdp\": \"C3\",\n",
    "    \"gcnn:tmdp+DFS\": \"C4\",\n",
    "    \"gcnn:tmdp+ObjLim\": \"C8\",\n",
    "    # \"gcnn:2022-12-26--00.53.24--best_params--tmdp+DFS.pkl\": \"C6\",\n",
    "}\n",
    "\n",
    "# rlbnb and Scavuzzo have different naming conventions\n",
    "rlbnb_colors = {\n",
    "    \"internal:vanillafullstrong\": \"k\",  # \"internal:vanillafullstrong\"\n",
    "    \"random\": \"C9\",  # random branching baseline\n",
    "    \"dqn\": \"C0\",  # RETRO by Parsonson et al. 2022\n",
    "    \"dqn_atoms\": \"C1\",  # DQN with head ensemble (50 atoms)\n",
    "    \"bipartite\": \"C5\",  # IL with bipartite observations\n",
    "    \"tripartite\": \"C6\",  # IL with tripartite observations\n",
    "    \"masked\": \"C7\",  # IL with bipartite observations with a mask on input\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c49670",
   "metadata": {},
   "source": [
    "Pick the table to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b060de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval on Scavuzzo's 100 test instances\n",
    "# filename = \"/Users/ivannazarov/Github/repos_with_rl/copt/rl2branch/cauctions_20221221-200625.csv\"\n",
    "\n",
    "# eval on OUR 1k instances w/o baseline heuristic\n",
    "# filename = \"/Users/ivannazarov/Github/repos_with_rl/copt/rl2branch/custom_20221222-021818.csv\"\n",
    "\n",
    "# eval on OUR 1k instances with baseline heuristic\n",
    "filename = (\n",
    "    #     \"/Users/ivannazarov/Github/repos_with_rl/copt/rl2branch/indset_20221223-112911.csv\"\n",
    "    \"/Users/ivannazarov/Github/repos_with_rl/copt/rl2branch/cauc_20221222-222137.csv\"\n",
    "    #     \"/Users/ivannazarov/Github/repos_with_rl/copt/rl2branch/setcover_20221223-125129.csv\"\n",
    "    #     \"/Users/ivannazarov/Github/repos_with_rl/copt/rl2branch/ufacilities_20221223-141643.csv\"\n",
    "    #     \"/Users/ivannazarov/Github/repos_with_rl/copt/rl2branch/indset_from_cauc_20221225-215555.csv\"\n",
    "    # XXX it looks like cuac traind model works just as well as the indset one\n",
    "    #     \"/Users/ivannazarov/Github/repos_with_rl/copt/rl2branch/cauc-retrained.csv\"\n",
    "    # XXX retrained seems to have replicated the original tmdp+dfs\n",
    ")\n",
    "metrics_scavuzzo = load_metrics_scavuzzo(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2203e3ae",
   "metadata": {},
   "source": [
    "Load the data from rlbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e8158",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_rlbnb = load_metrics_rlbnb(\n",
    "    \"/Users/ivannazarov/Github/repos_with_rl/copt/rlbnb/\"\n",
    "    \"results/combinatorial_auction_n_items_100_n_bids_500/\"\n",
    ")\n",
    "\n",
    "metrics = {**metrics_rlbnb, **metrics_scavuzzo}\n",
    "colors = {\n",
    "    **rlbnb_colors,\n",
    "    **scavuzzo_colors,\n",
    "}  # , \"gcnn:2022-12-26--00.53.24--best_params--tmdp+DFS.pkl\": \"C4\"}\n",
    "\n",
    "# metrics = metrics_rlbnb\n",
    "# colors = rlbnb_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c68da",
   "metadata": {},
   "source": [
    "Build the pp\n",
    "* `n_nodes`\n",
    "* `n_lpiter`\n",
    "* `f_soltime`\n",
    "* `f_gap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bcd992",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = \"n_nodes\"\n",
    "\n",
    "# base = \"internal:relpscost\"  # very strong\n",
    "base = \"internal:vanillafullstrong\"\n",
    "# base = \"gcnn:il\"  # \"internal:vanillafullstrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db09a0c",
   "metadata": {},
   "source": [
    "Build the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = {}\n",
    "for (k1, k2), v in metrics.items():\n",
    "    if k1 in colors:\n",
    "        out = metric.setdefault(k1, {})\n",
    "        out[k2] = v[series]\n",
    "\n",
    "pooled = {k: np.median(list(v.values()), 0) for k, v in metric.items()}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5), dpi=200)\n",
    "for name, repl in metric.items():\n",
    "    if base == name:\n",
    "        continue\n",
    "\n",
    "    # plot the mean pp curve first, and then individual pp-s\n",
    "    p, q = pp_curve(x=pooled[base], y=pooled[name], num=None)\n",
    "    ax.plot(p, q, label=name, c=colors[name])\n",
    "    for _, data in repl.items():\n",
    "        p, q = pp_curve(x=pooled[base], y=data, num=None)\n",
    "        ax.plot(p, q, c=colors[name], alpha=0.15, zorder=-10)\n",
    "\n",
    "ax.plot((0, 1), (0, 1), c=colors[base], zorder=10, alpha=0.25, label=base)\n",
    "ax.set_xlim(-0.025, 1.025)\n",
    "ax.set_ylim(-0.025, 1.025)\n",
    "ax.set_aspect(1.0)\n",
    "ax.legend(loc=\"best\", fontsize=\"xx-small\")\n",
    "\n",
    "# fig.savefig(f\"dump/tmdp__{os.path.basename(filename)}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5cd7b4",
   "metadata": {},
   "source": [
    "Now print the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = f'{\"name\":<26} {\"tot\":>8} {\"median\":>8} {\"sgm\":>8} {\"mean\":>8} {\"std\":>8}'\n",
    "row = \"{nom:<26} {tot:>8.0f} {median:>8.0f} {sgm:>8.0f} {mean:>8.0f} {std:>8.0f}\"\n",
    "\n",
    "print(header, \"\\n\" + \"-\" * len(header))\n",
    "for name, stat in get_stats(metrics, series).items():\n",
    "    print(row.format(nom=name, **stat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7409c3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
